{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "criteo2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEaLG5g2nIcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install google-cloud-bigquery-storage\n",
        "!pip install tensorflow-io==0.9.0\n",
        "!pip install --upgrade grpcio\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdy39F-JnNE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mfI4_0KnOK_",
        "colab_type": "code",
        "outputId": "125b8b56-d60c-4ac4-d517-c043840b0759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import *\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow_io.bigquery import BigQueryClient\n",
        "from tensorflow_io.bigquery import BigQueryReadSession\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "LOCATION = 'us'\n",
        "PROJECT_ID = \"alekseyv-scalableai-dev\"\n",
        "\n",
        "# # Download options.\n",
        "# DATA_URL = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle.tar.gz'\n",
        "\n",
        "# DATASET_ID = 'criteo_kaggle'\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "CSV_SCHEMA = [\n",
        "      bigquery.SchemaField(\"label\", \"INTEGER\", mode='REQUIRED'),\n",
        "      bigquery.SchemaField(\"int1\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int2\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int3\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int4\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int5\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int6\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int7\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int8\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int9\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int10\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int11\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int12\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"int13\", \"INTEGER\"),\n",
        "      bigquery.SchemaField(\"cat1\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat2\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat3\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat4\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat5\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat6\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat7\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat8\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat9\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat10\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat11\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat12\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat13\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat14\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat15\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat16\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat17\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat18\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat19\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat20\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat21\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat22\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat23\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat24\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat25\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"cat26\", \"STRING\")\n",
        "  ]\n",
        "\n",
        "def create_bigquery_dataset_if_necessary(dataset_id):\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset = bigquery.Dataset(bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id))\n",
        "  dataset.location = LOCATION\n",
        "\n",
        "  try:\n",
        "    dataset = client.create_dataset(dataset)  # API request\n",
        "    return True\n",
        "  except GoogleAPIError as err:\n",
        "    if err.code != 409: # http_client.CONFLICT\n",
        "      raise\n",
        "  return False\n",
        "\n",
        "def load_data_into_bigquery(url, dataset_id, table_id):\n",
        "  create_bigquery_dataset_if_necessary(dataset_id)\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset_ref = client.dataset(dataset_id)\n",
        "  table_ref = dataset_ref.table(table_id)\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "  job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "  job_config.source_format = bigquery.SourceFormat.CSV\n",
        "  #job_config.autodetect = True\n",
        "  job_config.schema = CSV_SCHEMA\n",
        "\n",
        "  load_job = client.load_table_from_uri(\n",
        "      url, table_ref, job_config=job_config\n",
        "  )\n",
        "  print(\"Starting job {}\".format(load_job.job_id))\n",
        "\n",
        "  load_job.result()  # Waits for table load to complete.\n",
        "  print(\"Job finished.\")\n",
        "\n",
        "  destination_table = client.get_table(table_ref)\n",
        "  print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
        "\n",
        "def get_mean_and_std_dicts():\n",
        "  #client = bigquery.Client(location=\"US\", project=PROJECT_ID)\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  query = \"\"\"\n",
        "    select\n",
        "    AVG(int1) as avg_int1, STDDEV(int1) as std_int1,\n",
        "    AVG(int2) as avg_int2, STDDEV(int2) as std_int2,\n",
        "    AVG(int3) as avg_int3, STDDEV(int3) as std_int3,\n",
        "    AVG(int4) as avg_int4, STDDEV(int4) as std_int4,\n",
        "    AVG(int5) as avg_int5, STDDEV(int5) as std_int5,\n",
        "    AVG(int6) as avg_int6, STDDEV(int6) as std_int6,\n",
        "    AVG(int7) as avg_int7, STDDEV(int7) as std_int7,\n",
        "    AVG(int8) as avg_int8, STDDEV(int8) as std_int8,\n",
        "    AVG(int9) as avg_int9, STDDEV(int9) as std_int9,\n",
        "    AVG(int10) as avg_int10, STDDEV(int10) as std_int10,\n",
        "    AVG(int11) as avg_int11, STDDEV(int11) as std_int11,\n",
        "    AVG(int12) as avg_int12, STDDEV(int12) as std_int12,\n",
        "    AVG(int13) as avg_int13, STDDEV(int13) as std_int13\n",
        "    from `alekseyv-scalableai-dev.criteo_kaggle.days`\n",
        "  \"\"\"\n",
        "  query_job = client.query(\n",
        "      query,\n",
        "      location=\"US\",\n",
        "  )  # API request - starts the query\n",
        "\n",
        "  df = query_job.to_dataframe()\n",
        "  mean_dict = dict((field[0].replace('avg_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('avg'))\n",
        "  std_dict = dict((field[0].replace('std_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('std'))\n",
        "  return (mean_dict, std_dict)\n",
        "\n",
        "def transofrom_row(row_dict, mean_dict, std_dict):\n",
        "  dict_without_label = row_dict.copy()\n",
        "  label = dict_without_label.pop('label')\n",
        "  for field in CSV_SCHEMA:\n",
        "    if (field.name.startswith('int')):\n",
        "      dict_without_label[field.name] = float((dict_without_label[field.name] - mean_dict[field.name]) / std_dict[field.name])\n",
        "  return (dict_without_label, label)\n",
        "\n",
        "def read_bigquery(dataset_id, table_name):\n",
        "\n",
        "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
        "  tensorflow_io_bigquery_client = BigQueryClient()\n",
        "  read_session = tensorflow_io_bigquery_client.read_session(\n",
        "      \"projects/\" + PROJECT_ID,\n",
        "      PROJECT_ID, table_name, dataset_id,\n",
        "      list(field.name for field in CSV_SCHEMA),\n",
        "      list(dtypes.int64 if field.field_type == 'INTEGER'\n",
        "           else dtypes.string for field in CSV_SCHEMA),\n",
        "      requested_streams=10)\n",
        "\n",
        "  dataset = read_session.parallel_read_rows()\n",
        "  transformed_ds = dataset.map (lambda row: transofrom_row(row, mean_dict, std_dict))\n",
        "  return transformed_ds\n",
        "\n",
        "def get_vocabulary_size_dict():\n",
        "  client = bigquery.Client(location=\"US\", project=PROJECT_ID)\n",
        "  query = \"\"\"\n",
        "    SELECT\n",
        "    COUNT(DISTINCT cat1) as cat1,\n",
        "    COUNT(DISTINCT cat2) as cat2,\n",
        "    COUNT(DISTINCT cat3) as cat3,\n",
        "    COUNT(DISTINCT cat4) as cat4,\n",
        "    COUNT(DISTINCT cat5) as cat5,\n",
        "    COUNT(DISTINCT cat6) as cat6,\n",
        "    COUNT(DISTINCT cat7) as cat7,\n",
        "    COUNT(DISTINCT cat8) as cat8,\n",
        "    COUNT(DISTINCT cat9) as cat9,\n",
        "    COUNT(DISTINCT cat10) as cat10,\n",
        "    COUNT(DISTINCT cat11) as cat11,\n",
        "    COUNT(DISTINCT cat12) as cat12,\n",
        "    COUNT(DISTINCT cat13) as cat13,\n",
        "    COUNT(DISTINCT cat14) as cat14,\n",
        "    COUNT(DISTINCT cat15) as cat15,\n",
        "    COUNT(DISTINCT cat16) as cat16,\n",
        "    COUNT(DISTINCT cat17) as cat17,\n",
        "    COUNT(DISTINCT cat18) as cat18,\n",
        "    COUNT(DISTINCT cat19) as cat19,\n",
        "    COUNT(DISTINCT cat20) as cat20,\n",
        "    COUNT(DISTINCT cat21) as cat21,\n",
        "    COUNT(DISTINCT cat22) as cat22,\n",
        "    COUNT(DISTINCT cat23) as cat23,\n",
        "    COUNT(DISTINCT cat24) as cat24,\n",
        "    COUNT(DISTINCT cat25) as cat25,\n",
        "    COUNT(DISTINCT cat26) as cat26\n",
        "    FROM\n",
        "      `alekseyv-scalableai-dev.criteo_kaggle.days`\n",
        "  \"\"\"\n",
        "  query_job = client.query(\n",
        "      query,\n",
        "      location=\"US\",\n",
        "  )  # API request - starts the query\n",
        "\n",
        "  df = query_job.to_dataframe()\n",
        "  dictionary = dict((field[0], df[field[0]][0]) for field in df.items())\n",
        "  return dictionary\n",
        "\n",
        "def create_categorical_feature_column(categorical_vocabulary_size_dict, key):\n",
        "  hash_bucket_size = min(categorical_vocabulary_size_dict[key], 1000)\n",
        "  # TODO: consider using categorical_column_with_vocabulary_list\n",
        "  categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    key,\n",
        "    hash_bucket_size,\n",
        "    dtype=tf.dtypes.string\n",
        "  )\n",
        "  if hash_bucket_size < 10:\n",
        "    return tf.feature_column.indicator_column(categorical_feature_column)\n",
        "\n",
        "  embedding_feature_column = tf.feature_column.embedding_column(\n",
        "      categorical_feature_column,\n",
        "      int(math.floor(6 * hash_bucket_size**0.25)))\n",
        "  return embedding_feature_column\n",
        "\n",
        "def create_feature_columns(categorical_vocabulary_size_dict):\n",
        "  feature_columns = []\n",
        "  feature_columns.extend(list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32)  for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label'))\n",
        "  feature_columns.extend(list(create_categorical_feature_column(categorical_vocabulary_size_dict, key) for key, _ in categorical_vocabulary_size_dict.items()))\n",
        "  return feature_columns\n",
        "\n",
        "def main(argv):\n",
        "#   if len(argv) > 1:\n",
        "#     raise app.UsageError(\"Too many command-line arguments.\")\n",
        "\n",
        "    #tf.debugging.set_log_device_placement(True)\n",
        "    print(\"tf.config.experimental.list_logical_devices(GPU): \" + str(tf.config.experimental.list_logical_devices('GPU')))\n",
        "    print(\"tf.config.experimental.list_physical_devices(GPU): \" + str(tf.config.experimental.list_physical_devices('GPU')))\n",
        "    print(\"device_lib.list_local_devices(): \" + str(device_lib.list_local_devices()))\n",
        "    print(\"tf.test.is_gpu_available(): \" + str(tf.test.is_gpu_available()))\n",
        "\n",
        "    print(\"reading categorical_vocabulary_size_dict\")\n",
        "    categorical_vocabulary_size_dict = get_vocabulary_size_dict()\n",
        "\n",
        "    feature_columns = create_feature_columns(categorical_vocabulary_size_dict)\n",
        "    print(\"categorical_vocabulary_size_dict: \" + str(categorical_vocabulary_size_dict))\n",
        "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "    [\n",
        "        feature_layer,\n",
        "        Dense(2560, activation=tf.nn.relu),\n",
        "        Dense(1024, activation=tf.nn.relu),\n",
        "        Dense(256, activation=tf.nn.relu),\n",
        "        Dense(1, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "\n",
        "    # Compile Keras model\n",
        "    model.compile(\n",
        "        optimizer=tf.optimizers.Adagrad(learning_rate=0.01),\n",
        "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    training_ds = read_bigquery('criteo_kaggle','days').take(1000000).shuffle(10000).batch(BATCH_SIZE)\n",
        "\n",
        "    model_dir = \"model2\"\n",
        "    if not os.path.exists(model_dir + \"/checkpoints\"):\n",
        "        os.makedirs(model_dir + \"/checkpoints\")\n",
        "\n",
        "    log_dir= model_dir + \"/logs\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    filepath= model_dir + \"/checkpoints/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, mode='max')\n",
        "\n",
        "    model.fit(training_ds, epochs=20,\n",
        "    callbacks=[tensorboard_callback, checkpoint]\n",
        "    )\n",
        "    #, callbacks=[tensorboard_callback, checkpoint]\n",
        "\n",
        "\n",
        "main(0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.config.experimental.list_logical_devices(GPU): [LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU')]\n",
            "tf.config.experimental.list_physical_devices(GPU): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "device_lib.list_local_devices(): [name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 190548185764725042\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 10292569199473284740\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 11450562686314358939\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 6363607175413675570\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "tf.test.is_gpu_available(): True\n",
            "reading categorical_vocabulary_size_dict\n",
            "categorical_vocabulary_size_dict: {'cat1': 1460, 'cat2': 583, 'cat3': 10131226, 'cat4': 2202607, 'cat5': 305, 'cat6': 23, 'cat7': 12517, 'cat8': 633, 'cat9': 3, 'cat10': 93145, 'cat11': 5683, 'cat12': 8351592, 'cat13': 3194, 'cat14': 27, 'cat15': 14992, 'cat16': 5461305, 'cat17': 10, 'cat18': 5652, 'cat19': 2172, 'cat20': 3, 'cat21': 7046546, 'cat22': 17, 'cat23': 15, 'cat24': 286180, 'cat25': 104, 'cat26': 142571}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_io/bigquery/python/ops/bigquery_api.py:214: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "Epoch 1/20\n",
            "      1/Unknown - 11s 11s/step - loss: 0.6931 - accuracy: 0.5723WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (5.455854). Check your callbacks.\n",
            "      2/Unknown - 24s 12s/step - loss: 0.6931 - accuracy: 0.5752WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.258752). Check your callbacks.\n",
            "   1954/Unknown - 309s 158ms/step - loss: 0.6836 - accuracy: 0.5712"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c97dee07979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6c97dee07979>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     model.fit(training_ds, epochs=20,\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m#, callbacks=[tensorboard_callback, checkpoint]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m                       total_epochs=1)\n\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 372\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_log_weights\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m   1692\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m           \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_weight_as_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, tensor, family, step)\u001b[0m\n\u001b[1;32m    799\u001b[0m         name=scope)\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary_writer_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36msummary_writer_function\u001b[0;34m(name, tensor, function, family)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     op = smart_cond.smart_cond(\n\u001b[0;32m--> 730\u001b[0;31m         should_record_summaries(), record, _nothing, name=\"\")\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m()\u001b[0m\n\u001b[1;32m    721\u001b[0m     with ops.name_scope(name_scope), summary_op_util.summary_scope(\n\u001b[1;32m    722\u001b[0m         name, family, values=[tensor]) as (tag, scope):\n\u001b[0;32m--> 723\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(tag, scope)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         name=scope)\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary_writer_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_histogram_summary\u001b[0;34m(writer, step, tag, values, name)\u001b[0m\n\u001b[1;32m    584\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         return write_histogram_summary_eager_fallback(\n\u001b[0;32m--> 586\u001b[0;31m             writer, step, tag, values, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    587\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_histogram_summary_eager_fallback\u001b[0;34m(writer, step, tag, values, name, ctx)\u001b[0m\n\u001b[1;32m    620\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   _result = _execute.execute(b\"WriteHistogramSummary\", 0, inputs=_inputs_flat,\n\u001b[0;32m--> 622\u001b[0;31m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    623\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: sequential/dense/kernel_0 [Op:WriteHistogramSummary] name: sequential/dense/kernel_0/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1uApKdWAwV_",
        "colab_type": "code",
        "outputId": "e16920ee-508b-40a4-9815-520db2d56885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.losses.BinaryCrossentropy at 0x7feee3830d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}