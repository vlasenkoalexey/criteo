{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train-location {local,cloud}]\n",
      "                             [--model-name MODEL_NAME] --job-dir JOB_DIR\n",
      "                             [--distribution-strategy {tf.distribute.MirroredStrategy,tf.distribute.experimental.ParameterServerStrategy,tf.distribute.experimental.MultiWorkerMirroredStrategy,tf.distribute.experimental.CentralStorageStrategy,tf.distribute.experimental.TPUStrategy,tf.distribute.OneDeviceStrategy}]\n",
      "                             [--training-function {train_keras_sequential,train_keras_functional,train_keras_functional_wide_and_deep,train_keras_to_estimator_functional,train_keras_to_estimator_sequential,train_estimator,train_estimator_wide_and_deep,train_keras_functional_no_feature_layer,train_custom_loop_keras_sequential,train_custom_loop_keras_model_functional_no_feature_layer}]\n",
      "                             [--batch-size BATCH_SIZE]\n",
      "                             [--dataset-size {full,small}]\n",
      "                             [--dataset-source {bq,gcs}]\n",
      "                             [--num-epochs NUM_EPOCHS]\n",
      "                             [--embeddings-mode {n,o,n,e, ,m,a,n,u,a,l, ,h,a,s,h,b,u,c,k,e,t, ,v,o,c,a,b,u,l,a,r}]\n",
      "                             [--tensorboard]\n",
      "                             [--ai-platform-mode AI_PLATFORM_MODE]\n",
      "                             [--image-name IMAGE_NAME] [--no-gpu]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --job-dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow_io as tf_io\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from tensorflow.python.data.experimental.ops import interleave_ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import google.cloud.logging\n",
    "\n",
    "import argparse\n",
    "\n",
    "class BatchAccuracyAndLossSummaryCallback(tf.keras.callbacks.Callback):\n",
    "  # TODO: make it dist. strat. compartible\n",
    "  def __init__(self, dataset_size):\n",
    "    # Callback should only write summaries on the chief when in a Multi-Worker setting.\n",
    "    self._chief_worker_only = True\n",
    "    self.update_freq = 50 if dataset_size == 'small' else 1000\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    self.epoch = epoch\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    if batch % self.update_freq == 0:\n",
    "      if 'accuracy' in logs:\n",
    "        tf.summary.scalar('accuracy', logs['accuracy'], batch, description='epoch: {}'.format(self.epoch))\n",
    "        tf.summary.scalar('accuracy epoch: {}'.format(self.epoch), logs['accuracy'], batch, description='epoch: {}'.format(self.epoch))\n",
    "      if 'loss' in logs:\n",
    "        tf.summary.scalar('loss', logs['loss'], batch, description='epoch: {}'.format(self.epoch))\n",
    "        tf.summary.scalar('loss epoch: {}'.format(self.epoch), logs['loss'], batch, description='epoch: {}'.format(self.epoch))\n",
    "\n",
    "class TrainTimeCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    self.epoch_start_time = datetime.datetime.now()\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    logging.info('\\nepoch train time: (hh:mm:ss.ms) {}'.format(datetime.datetime.now() - self.epoch_start_time))\n",
    "    if not self.params is None:\n",
    "      if 'steps' in self.params:\n",
    "        epoch_milliseconds = (datetime.datetime.now() - self.epoch_start_time).total_seconds() * 1000\n",
    "        logging.info('{} ms/step'.format(epoch_milliseconds / self.params['steps']))\n",
    "      if BATCH_SIZE is not None:\n",
    "        logging.info('{} microseconds/example'.format(1000 * epoch_milliseconds  / self.params['steps'] / BATCH_SIZE))\n",
    "\n",
    "  def on_train_begin(self, logs=None):\n",
    "    self.start_training_time = datetime.datetime.now()\n",
    "\n",
    "  def on_train_end(self, logs=None):\n",
    "    logging.info('total train time: (hh:mm:ss.ms) {}'.format(datetime.datetime.now() - self.start_training_time))\n",
    "\n",
    "LOCATION = 'us'\n",
    "PROJECT_ID = \"alekseyv-scalableai-dev\" # TODO: replace with your project name\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"alekseyv-scalableai-dev-077efe757ef6.json\" # TODO: replace with your key name\n",
    "GOOGLE_APPLICATION_CREDENTIALS_GCS_BUCKET = 'gs://alekseyv-scalableai-dev-private-bucket/criteo' # TODO: replace with the path to the GCS bucket your project has access to\n",
    "\n",
    "DATASET_ID = 'criteo_kaggle'\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "EMBEDDINGS_MODE_TYPE_VALUES = 'none manual hashbucket vocabular'\n",
    "EMBEDDINGS_MODE_TYPE = Enum('EMBEDDINGS_MODE_TYPE', 'none manual hashbucket vocabular')\n",
    "EMBEDDINGS_MODE = EMBEDDINGS_MODE_TYPE.hashbucket\n",
    "\n",
    "FULL_TRAIN_DATASET_SIZE = 36670642 # select count(1) from `alekseyv-scalableai-dev.criteo_kaggle.train`\n",
    "SMALL_TRAIN_DATASET_SIZE = 366715  # select count(1) from `alekseyv-scalableai-dev.criteo_kaggle.train_small`\n",
    "\n",
    "TRAIN_LOCATION_TYPE_VALUES = 'local cloud'\n",
    "TRAIN_LOCATION_TYPE = Enum('TRAIN_LOCATION_TYPE', TRAIN_LOCATION_TYPE_VALUES)\n",
    "TRAIN_LOCATION = TRAIN_LOCATION_TYPE.local\n",
    "\n",
    "# https://www.tensorflow.org/guide/distributed_training\n",
    "DISTRIBUTION_STRATEGY_TYPE = None\n",
    "DISTRIBUTION_STRATEGY_TYPE_VALUES = 'tf.distribute.MirroredStrategy tf.distribute.experimental.ParameterServerStrategy ' \\\n",
    "  'tf.distribute.experimental.MultiWorkerMirroredStrategy tf.distribute.experimental.CentralStorageStrategy ' \\\n",
    "  'tf.distribute.experimental.TPUStrategy tf.distribute.OneDeviceStrategy'\n",
    "TRAINING_FUNCTION_VALUES = 'train_keras_sequential train_keras_functional train_keras_functional_wide_and_deep ' \\\n",
    "  'train_keras_to_estimator_functional train_keras_to_estimator_sequential train_estimator train_estimator_wide_and_deep ' \\\n",
    "  'train_keras_functional_no_feature_layer train_custom_loop_keras_sequential train_custom_loop_keras_model_functional_no_feature_layer'\n",
    "\n",
    "DATASET_SIZE_TYPE = Enum('DATASET_SIZE_TYPE', 'full small')\n",
    "DATASET_SIZE = DATASET_SIZE_TYPE.small\n",
    "\n",
    "DATASET_SOURCE_TYPE = Enum('DATASET_SOURCE_TYPE', 'bq gcs')\n",
    "DATASET_SOURCE = DATASET_SOURCE_TYPE.bq\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "      bigquery.SchemaField(\"label\", \"INTEGER\", mode='REQUIRED'),\n",
    "      bigquery.SchemaField(\"int1\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int2\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int3\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int4\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int5\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int6\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int7\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int8\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int9\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int10\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int11\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int12\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int13\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"cat1\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat2\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat3\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat4\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat5\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat6\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat7\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat8\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat9\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat10\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat11\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat12\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat13\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat14\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat15\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat16\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat17\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat18\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat19\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat20\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat21\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat22\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat23\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat24\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat25\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat26\", \"STRING\")\n",
    "  ]\n",
    "\n",
    "def get_mean_and_std_dicts():\n",
    "  table_name = 'days' if DATASET_SIZE == DATASET_SIZE_TYPE.full else 'small'\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  query = \"\"\"\n",
    "    select\n",
    "    AVG(int1) as avg_int1, STDDEV(int1) as std_int1,\n",
    "    AVG(int2) as avg_int2, STDDEV(int2) as std_int2,\n",
    "    AVG(int3) as avg_int3, STDDEV(int3) as std_int3,\n",
    "    AVG(int4) as avg_int4, STDDEV(int4) as std_int4,\n",
    "    AVG(int5) as avg_int5, STDDEV(int5) as std_int5,\n",
    "    AVG(int6) as avg_int6, STDDEV(int6) as std_int6,\n",
    "    AVG(int7) as avg_int7, STDDEV(int7) as std_int7,\n",
    "    AVG(int8) as avg_int8, STDDEV(int8) as std_int8,\n",
    "    AVG(int9) as avg_int9, STDDEV(int9) as std_int9,\n",
    "    AVG(int10) as avg_int10, STDDEV(int10) as std_int10,\n",
    "    AVG(int11) as avg_int11, STDDEV(int11) as std_int11,\n",
    "    AVG(int12) as avg_int12, STDDEV(int12) as std_int12,\n",
    "    AVG(int13) as avg_int13, STDDEV(int13) as std_int13\n",
    "    from `alekseyv-scalableai-dev.criteo_kaggle.{table_name}`\n",
    "  \"\"\".format(table_name = table_name)\n",
    "  query_job = client.query(\n",
    "      query,\n",
    "      location=LOCATION,\n",
    "  )  # API request - starts the query\n",
    "\n",
    "  df = query_job.to_dataframe()\n",
    "\n",
    "  mean_dict = dict((field[0].replace('avg_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('avg'))\n",
    "  std_dict = dict((field[0].replace('std_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('std'))\n",
    "  return (mean_dict, std_dict)\n",
    "\n",
    "def get_vocabulary_size_dict():\n",
    "  table_name = 'days' if DATASET_SIZE == DATASET_SIZE_TYPE.full else 'small'\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  query = \"\"\"\n",
    "    SELECT\n",
    "    COUNT(DISTINCT cat1) as cat1,\n",
    "    COUNT(DISTINCT cat2) as cat2,\n",
    "    COUNT(DISTINCT cat3) as cat3,\n",
    "    COUNT(DISTINCT cat4) as cat4,\n",
    "    COUNT(DISTINCT cat5) as cat5,\n",
    "    COUNT(DISTINCT cat6) as cat6,\n",
    "    COUNT(DISTINCT cat7) as cat7,\n",
    "    COUNT(DISTINCT cat8) as cat8,\n",
    "    COUNT(DISTINCT cat9) as cat9,\n",
    "    COUNT(DISTINCT cat10) as cat10,\n",
    "    COUNT(DISTINCT cat11) as cat11,\n",
    "    COUNT(DISTINCT cat12) as cat12,\n",
    "    COUNT(DISTINCT cat13) as cat13,\n",
    "    COUNT(DISTINCT cat14) as cat14,\n",
    "    COUNT(DISTINCT cat15) as cat15,\n",
    "    COUNT(DISTINCT cat16) as cat16,\n",
    "    COUNT(DISTINCT cat17) as cat17,\n",
    "    COUNT(DISTINCT cat18) as cat18,\n",
    "    COUNT(DISTINCT cat19) as cat19,\n",
    "    COUNT(DISTINCT cat20) as cat20,\n",
    "    COUNT(DISTINCT cat21) as cat21,\n",
    "    COUNT(DISTINCT cat22) as cat22,\n",
    "    COUNT(DISTINCT cat23) as cat23,\n",
    "    COUNT(DISTINCT cat24) as cat24,\n",
    "    COUNT(DISTINCT cat25) as cat25,\n",
    "    COUNT(DISTINCT cat26) as cat26\n",
    "    FROM\n",
    "      `alekseyv-scalableai-dev.criteo_kaggle.{table_name}`\n",
    "  \"\"\".format(table_name = table_name)\n",
    "  query_job = client.query(\n",
    "      query,\n",
    "      location=LOCATION,\n",
    "  )  # API request - starts the query\n",
    "\n",
    "  df = query_job.to_dataframe()\n",
    "  dictionary = dict((field[0], df[field[0]][0]) for field in df.items())\n",
    "  return dictionary\n",
    "\n",
    "def get_corpus_dict():\n",
    "  table_name = 'days' if DATASET_SIZE == DATASET_SIZE_TYPE.full else 'small'\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  query = \"\"\"\n",
    "    select\n",
    "    cat_name,\n",
    "    cat_value,\n",
    "    cat_index\n",
    "    from `alekseyv-scalableai-dev.criteo_kaggle.{table_name}_corpus`\n",
    "  \"\"\".format(table_name = table_name)\n",
    "  query_job = client.query(\n",
    "      query,\n",
    "      location=\"US\",\n",
    "  )  # API request - starts the query\n",
    "\n",
    "  df = query_job.to_dataframe()\n",
    "  corpus = dict()\n",
    "  for _, row in df.iterrows():\n",
    "    cat_name = row[0]\n",
    "    cat_value = row[1]\n",
    "    cat_index = row[2]\n",
    "    if not cat_name in corpus:\n",
    "      corpus[cat_name] = dict()\n",
    "    if cat_value is None:\n",
    "      cat_value = ''\n",
    "    corpus[cat_name][cat_value] = cat_index\n",
    "  return corpus\n",
    "\n",
    "def corpus_to_lookuptable(corpus):\n",
    "  lookup_dict = dict()\n",
    "  for key, value in corpus.items():\n",
    "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "      list(value.keys()),\n",
    "      list(value.values()),\n",
    "      key_dtype=tf.string,\n",
    "      value_dtype=tf.int64)\n",
    "    # cat_index in corpus starts with 1, reserving 0 for out of vocabulary values\n",
    "    lookup_table = tf.lookup.StaticHashTable(initializer, 0)\n",
    "    lookup_dict[key] = lookup_table\n",
    "  return lookup_dict\n",
    "\n",
    "def get_corpus():\n",
    "  if EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.manual or EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "    return corpus_to_lookuptable(get_corpus_dict())\n",
    "  else:\n",
    "    return dict()\n",
    "\n",
    "def transform_row(row_dict, mean_dict, std_dict, corpus):\n",
    "  dict_without_label = dict(row_dict)\n",
    "  label = dict_without_label.pop('label')\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] != 0:\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        else:\n",
    "            # use normalized mean value if data is missing\n",
    "            dict_without_label[field.name] = float(mean_dict[field.name] / std_dict[field.name])\n",
    "    elif field.name.startswith('cat'):\n",
    "      if EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.none:\n",
    "        dict_without_label.pop(field.name)\n",
    "      elif EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.manual:\n",
    "        cat = dict_without_label[field.name]\n",
    "        if cat is None:\n",
    "          cat = ''\n",
    "        cat_index = corpus[field.name].lookup(cat)\n",
    "        if cat_index is None:\n",
    "          tf.print('not found for {}'.format(field.name))\n",
    "          cat_index = tf.constant(-1)\n",
    "        dict_without_label[field.name] = cat_index\n",
    "  return (dict_without_label, label)\n",
    "\n",
    "def read_bigquery(table_name):\n",
    "  if DATASET_SIZE == DATASET_SIZE_TYPE.small:\n",
    "    table_name += '_small'\n",
    "\n",
    "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "  corpus = get_corpus()\n",
    "  requested_streams_count = 10\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + PROJECT_ID,\n",
    "      PROJECT_ID, table_name, DATASET_ID,\n",
    "      list(field.name for field in CSV_SCHEMA),\n",
    "      list(dtypes.int64 if field.field_type == 'INTEGER'\n",
    "           else dtypes.string for field in CSV_SCHEMA),\n",
    "      requested_streams=requested_streams_count)\n",
    "\n",
    "  # manually sharding output instaead of using return read_session.parallel_read_rows()\n",
    "  streams = read_session.get_streams()\n",
    "  # streams_count = len(streams) # does not work for Estimator\n",
    "  streams_count = tf.size(streams)\n",
    "  streams_count64 = tf.cast(streams_count, dtype=tf.int64)\n",
    "  streams_ds = dataset_ops.Dataset.from_tensor_slices(streams).shuffle(buffer_size=streams_count64)\n",
    "  dataset = streams_ds.interleave(\n",
    "            read_session.read_rows,\n",
    "            cycle_length=streams_count64,\n",
    "            num_parallel_calls=streams_count64)\n",
    "\n",
    "  transformed_ds = dataset.map (lambda row: transform_row(row, mean_dict, std_dict, corpus), num_parallel_calls=streams_count) \\\n",
    "    .shuffle(10000) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(100)\n",
    "\n",
    "  # TODO: enable once tf.data.experimental.AutoShardPolicy.OFF is available\n",
    "  # Interleave dataset is not shardable, turning off sharding\n",
    "  # See https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size\n",
    "  # Instead we are shuffling data.\n",
    "  # options = tf.data.Options()\n",
    "  #  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "  # return transformed_ds.with_options(options)\n",
    "  return transformed_ds\n",
    "\n",
    "def transofrom_row_gcs(row_tuple, mean_dict, std_dict, corpus):\n",
    "    row_dict = dict(zip(list(field.name for field in CSV_SCHEMA) + ['row_hash'], list(row_tuple)))\n",
    "    row_dict.pop('row_hash')\n",
    "    return transform_row(row_dict, mean_dict, std_dict, corpus)\n",
    "\n",
    "\n",
    "def _get_file_names(file_pattern):\n",
    "  if isinstance(file_pattern, list):\n",
    "    if not file_pattern:\n",
    "      raise ValueError(\"File pattern is empty.\")\n",
    "    file_names = []\n",
    "    for entry in file_pattern:\n",
    "      file_names.extend(gfile.Glob(entry))\n",
    "  else:\n",
    "    file_names = list(gfile.Glob(file_pattern))\n",
    "\n",
    "  if not file_names:\n",
    "    raise ValueError(\"No files match %s.\" % file_pattern)\n",
    "  return file_names\n",
    "\n",
    "def read_gcs(table_name):\n",
    "  if DATASET_SIZE == DATASET_SIZE_TYPE.small:\n",
    "    table_name += '_small'\n",
    "  else:\n",
    "    table_name += '_full'\n",
    "\n",
    "  gcs_filename_glob = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/{}*'.format(table_name)\n",
    "  file_names = _get_file_names(gcs_filename_glob)\n",
    "  num_parallel_calls = max(10, len(file_names))\n",
    "  file_names_ds = dataset_ops.Dataset.from_tensor_slices(file_names).shuffle(buffer_size=20)\n",
    "  record_defaults = list(tf.int32 if field.name == 'label' else tf.constant(0, dtype=tf.int32) if field.name.startswith('int') else tf.constant('', dtype=tf.string) for field in CSV_SCHEMA) + [tf.string]\n",
    "  dataset = file_names_ds.interleave(\n",
    "          lambda file_name: tf.data.experimental.CsvDataset(file_name, record_defaults, field_delim='\\t', header=False),\n",
    "          cycle_length=num_parallel_calls,\n",
    "          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "  corpus = get_corpus()\n",
    "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "  transformed_ds = dataset.map (lambda *row_tuple: transofrom_row_gcs(row_tuple, mean_dict, std_dict, corpus)) \\\n",
    "    .shuffle(10000) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(100)\n",
    "  return transformed_ds\n",
    "\n",
    "def get_dataset(table_name):\n",
    "  global DATASET_SOURCE\n",
    "  return read_gcs(table_name) if DATASET_SOURCE == DATASET_SOURCE_TYPE.gcs else read_bigquery(table_name)\n",
    "\n",
    "def get_max_steps():\n",
    "  dataset_size = FULL_TRAIN_DATASET_SIZE if DATASET_SIZE == DATASET_SIZE_TYPE.full else SMALL_TRAIN_DATASET_SIZE\n",
    "  return EPOCHS * dataset_size // BATCH_SIZE\n",
    "\n",
    "def create_categorical_feature_column_with_hash_bucket(corpus_dict, key):\n",
    "  corpus_size = len(corpus_dict[key])\n",
    "  hash_bucket_size = min(corpus_size, 100000)\n",
    "  categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key,\n",
    "    hash_bucket_size,\n",
    "    dtype=tf.dtypes.string\n",
    "  )\n",
    "  if hash_bucket_size < 10:\n",
    "    logging.info('categorical column %s hash_bucket_size %d - creating indicator column', key, hash_bucket_size)\n",
    "    return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "  embedding_dimension = int(min(50, math.floor(6 * hash_bucket_size**0.25)))\n",
    "  embedding_feature_column = tf.feature_column.embedding_column(\n",
    "      categorical_feature_column,\n",
    "      embedding_dimension)\n",
    "  logging.info('categorical column %s hash_bucket_size %d dimension %d', key, hash_bucket_size, embedding_dimension)\n",
    "  return embedding_feature_column\n",
    "\n",
    "def create_categorical_feature_column_with_vocabulary_list(corpus_dict, key):\n",
    "  corpus_size = len(corpus_dict[key])\n",
    "  categorical_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key,\n",
    "    list(corpus_dict[key].keys()),\n",
    "    dtype=tf.dtypes.string,\n",
    "    num_oov_buckets=corpus_size\n",
    "  )\n",
    "\n",
    "  embedding_dimension = int(min(50, math.floor(6 * corpus_size**0.25)))\n",
    "  embedding_feature_column = tf.feature_column.embedding_column(\n",
    "      categorical_feature_column,\n",
    "      embedding_dimension)\n",
    "  logging.info('categorical column %s corpus_size %d dimension %d', key, corpus_size, embedding_dimension)\n",
    "  return embedding_feature_column\n",
    "\n",
    "def create_linear_feature_columns():\n",
    "  return list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32)  for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "\n",
    "def create_categorical_feature_columns():\n",
    "  if EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.none:\n",
    "    return []\n",
    "  elif EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.hashbucket:\n",
    "    corpus_dict = get_corpus_dict()\n",
    "    return list(create_categorical_feature_column_with_hash_bucket(corpus_dict, key)\n",
    "      for key, _ in corpus_dict.items())\n",
    "  elif EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "    corpus_dict = get_corpus_dict()\n",
    "    return list(create_categorical_feature_column_with_vocabulary_list(corpus_dict, key)\n",
    "      for key, _ in corpus_dict.items())\n",
    "  else:\n",
    "    raise ValueError('invalid EMBEDDINGS_MODE: {}'.format(EMBEDDINGS_MODE))\n",
    "\n",
    "def create_feature_columns():\n",
    "  feature_columns = []\n",
    "  feature_columns.extend(create_linear_feature_columns())\n",
    "  feature_columns.extend(create_categorical_feature_columns())\n",
    "  return feature_columns\n",
    "\n",
    "def create_input_layer():\n",
    "    numeric_feature_columns = create_linear_feature_columns()\n",
    "    numerical_input_layers = {\n",
    "       feature_column.name: tf.keras.layers.Input(name=feature_column.name, shape=(1,), dtype=tf.float32)\n",
    "       for feature_column in numeric_feature_columns\n",
    "    }\n",
    "    categorical_feature_columns = create_categorical_feature_columns()\n",
    "    categorical_input_layers = {\n",
    "       feature_column.categorical_column.name: tf.keras.layers.Input(name=feature_column.categorical_column.name, shape=(), dtype=tf.string)\n",
    "       for feature_column in categorical_feature_columns\n",
    "    }\n",
    "    input_layers = numerical_input_layers.copy()\n",
    "    input_layers.update(categorical_input_layers)\n",
    "\n",
    "    return (input_layers, numeric_feature_columns + categorical_feature_columns)\n",
    "\n",
    "def create_embedding_from_input(corpus_dict, name, input_layer):\n",
    "  size = len(corpus_dict[name]) + 2\n",
    "  dimension =  int(min(50, math.floor(6 * size**0.25)))\n",
    "  logging.info('embedding name:{} size:{} dim:{}'.format(name, size, dimension))\n",
    "  embedding = tf.keras.layers.Embedding(size, dimension, name = name + '_embedding', input_length=1)(input_layer)\n",
    "  return embedding\n",
    "\n",
    "def create_keras_model_functional():\n",
    "    (feature_layer_inputs, feature_columns) = create_input_layer()\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    x = tf.keras.layers.Dense(2560, activation=tf.nn.relu)(feature_layer_outputs)\n",
    "    x = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=tf.nn.relu)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "    inputs=[v for v in feature_layer_inputs.values()]\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "      # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      #optimizer=tf.optimizers.Adam(),\n",
    "      #optimizer=tf.optimizers.Adagrad(),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "    logging.info(\"model: \" + str(model.summary()))\n",
    "    return model\n",
    "\n",
    "def create_keras_model_functional_no_feature_layer():\n",
    "  corpus_dict = get_corpus_dict()\n",
    "  categorical_input_with_names = list((field.name, tf.keras.layers.Input(shape=[1], name = field.name, dtype=tf.int32))\n",
    "    for field in CSV_SCHEMA if field.field_type == 'STRING' and field.name != 'label')\n",
    "  categorical_inputs = list(input_layer\n",
    "    for (name, input_layer) in categorical_input_with_names)\n",
    "  categorical_embeddings = list(create_embedding_from_input(corpus_dict, name, input_layer)\n",
    "    for (name, input_layer) in categorical_input_with_names)\n",
    "\n",
    "  numerical_inputs = list(tf.keras.layers.Input(shape=[1], name = field.name, dtype=tf.float32)\n",
    "    for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "\n",
    "  categorical_combined = tf.keras.layers.concatenate(categorical_embeddings)\n",
    "  x = tf.keras.layers.Flatten()(categorical_combined)\n",
    "  x = tf.keras.layers.concatenate([x] + numerical_inputs)\n",
    "  x = tf.keras.layers.Dense(2560, activation=tf.nn.relu)(x)\n",
    "  x = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(x)\n",
    "  x = tf.keras.layers.Dense(256, activation=tf.nn.relu)(x)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "  model = tf.keras.Model(inputs=categorical_inputs + numerical_inputs, outputs=outputs)\n",
    "\n",
    "  # Compile Keras model\n",
    "  model.compile(\n",
    "    # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "    #optimizer=tf.optimizers.Adam(),\n",
    "    #optimizer=tf.optimizers.Adagrad(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "  logging.info(\"model: \" + str(model.summary()))\n",
    "\n",
    "  return model\n",
    "\n",
    "def create_keras_model_functional_wide_and_deep():\n",
    "    (feature_layer_inputs, feature_columns) = create_input_layer()\n",
    "    categorical_feature_columns=create_categorical_feature_columns()\n",
    "\n",
    "    wide = tf.keras.layers.DenseFeatures(categorical_feature_columns)(feature_layer_inputs)\n",
    "\n",
    "    deep = tf.keras.layers.DenseFeatures(feature_columns)(feature_layer_inputs)\n",
    "    deep = tf.keras.layers.Dense(2560, activation=tf.nn.relu)(deep)\n",
    "    deep = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(deep)\n",
    "    deep = tf.keras.layers.Dense(256, activation=tf.nn.relu)(deep)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(\n",
    "      tf.keras.layers.concatenate([deep, wide]))\n",
    "\n",
    "    outputs = tf.squeeze(outputs, -1)\n",
    "    inputs=[v for v in feature_layer_inputs.values()]\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "      # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "      #optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "    logging.info(\"model: \" + str(model.summary()))\n",
    "    return model\n",
    "\n",
    "def create_keras_model_sequential():\n",
    "  feature_columns = create_feature_columns()\n",
    "\n",
    "  feature_layer = tf.keras.layers.DenseFeatures(feature_columns, name=\"feature_layer\")\n",
    "  Dense = tf.keras.layers.Dense\n",
    "  model = tf.keras.Sequential(\n",
    "  [\n",
    "      feature_layer,\n",
    "      Dense(2560, activation=tf.nn.relu),\n",
    "      Dense(1024, activation=tf.nn.relu),\n",
    "      Dense(256, activation=tf.nn.relu),\n",
    "      Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "  logging.info('compiling sequential keras model')\n",
    "  # Compile Keras model\n",
    "  model.compile(\n",
    "      # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "      #optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train_and_evaluate_keras_model(model, model_dir):\n",
    "  dataset_size = FULL_TRAIN_DATASET_SIZE if DATASET_SIZE == DATASET_SIZE_TYPE.full else SMALL_TRAIN_DATASET_SIZE\n",
    "  logging.info('training datset size: {}'.format(dataset_size))\n",
    "  training_ds = get_dataset('train')\n",
    "\n",
    "  #log_dir= os.path.join(model_dir, \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  log_dir= os.path.join(model_dir, \"logs\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, embeddings_freq=1, profile_batch=1)\n",
    "\n",
    "  checkpoints_dir= os.path.join(model_dir, \"checkpoints\")\n",
    "  # crashing https://github.com/tensorflow/tensorflow/issues/27688\n",
    "  if not os.path.exists(checkpoints_dir):\n",
    "      os.makedirs(checkpoints_dir)\n",
    "\n",
    "  callbacks=[]\n",
    "  train_time_callback = TrainTimeCallback()\n",
    "\n",
    "  if DISTRIBUTION_STRATEGY_TYPE == 'tf.distribute.experimental.TPUStrategy':\n",
    "    # epoch and accuracy constants are not supported when training on TPU.\n",
    "    checkpoints_file_path = checkpoints_dir + \"/epochs_tpu.hdf5\"\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoints_file_path, verbose=1, mode='max')\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "  else:\n",
    "    if EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.manual or DISTRIBUTION_STRATEGY_TYPE == 'tf.distribute.experimental.MultiWorkerMirroredStrategy':\n",
    "      # accuracy fails for adagrad\n",
    "      # for some reason accuracy is not available for EMBEDDINGS_MODE_TYPE.manual\n",
    "      # for some reason accuracy is not available for MultiWorkerMirroredStrategy\n",
    "      checkpoints_file_path = checkpoints_dir + \"/epochs:{epoch:03d}.hdf5\"\n",
    "    else:\n",
    "      checkpoints_file_path = checkpoints_dir + \"/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoints_file_path, verbose=1, mode='max')\n",
    "    file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "    file_writer.set_as_default()\n",
    "    batch_summary_callback = BatchAccuracyAndLossSummaryCallback(DATASET_SIZE)\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback, batch_summary_callback, train_time_callback]\n",
    "    #callbacks=[tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "\n",
    "  verbosity = 1 if TRAIN_LOCATION == TRAIN_LOCATION_TYPE.local else 2\n",
    "  logging.info('training keras model')\n",
    "  model.fit(training_ds, epochs=EPOCHS, verbose=verbosity, callbacks=callbacks)\n",
    "  eval_ds = get_dataset('test')\n",
    "  logging.info(\"done training keras model, evaluating model\")\n",
    "  loss, accuracy = model.evaluate(eval_ds, verbose=verbosity)\n",
    "  logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "  logging.info(\"done evaluating keras model\")\n",
    "\n",
    "def train_keras_model_to_estimator(strategy, model, model_dir):\n",
    "    logging.info('training for {} steps'.format(get_max_steps()))\n",
    "    config = tf.estimator.RunConfig(\n",
    "            train_distribute=strategy,\n",
    "            eval_distribute=strategy)\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model, model_dir=model_dir, config=config)\n",
    "    # Need to specify both max_steps and epochs. Each worker will go through epoch separately.\n",
    "    # see https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate?version=stable\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        keras_estimator,\n",
    "        train_spec=tf.estimator.TrainSpec(input_fn=lambda: get_dataset('train').repeat(EPOCHS), max_steps=get_max_steps()),\n",
    "        eval_spec=tf.estimator.EvalSpec(input_fn=lambda: get_dataset('test')))\n",
    "\n",
    "def train_keras_sequential(strategy, model_dir):\n",
    "  train_and_evaluate_keras_model(create_keras_model_sequential(), model_dir)\n",
    "\n",
    "def train_keras_functional(strategy, model_dir):\n",
    "  train_and_evaluate_keras_model(create_keras_model_functional(), model_dir)\n",
    "\n",
    "def train_keras_functional_no_feature_layer(strategy, model_dir):\n",
    "  train_and_evaluate_keras_model(create_keras_model_functional_no_feature_layer(), model_dir)\n",
    "\n",
    "def train_keras_functional_wide_and_deep(strategy, model_dir):\n",
    "  train_and_evaluate_keras_model(create_keras_model_functional_wide_and_deep(), model_dir)\n",
    "\n",
    "def train_keras_to_estimator_sequential(strategy, model_dir):\n",
    "  train_keras_model_to_estimator(strategy, create_keras_model_sequential(), model_dir)\n",
    "\n",
    "def train_keras_to_estimator_functional(strategy, model_dir):\n",
    "  train_keras_model_to_estimator(strategy, create_keras_model_functional(), model_dir)\n",
    "\n",
    "def train_estimator(strategy, model_dir):\n",
    "  logging.info('training for {} steps'.format(get_max_steps()))\n",
    "  config = tf.estimator.RunConfig(\n",
    "          train_distribute=strategy,\n",
    "          eval_distribute=strategy)\n",
    "  feature_columns = create_feature_columns()\n",
    "  estimator = tf.estimator.DNNClassifier(\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      feature_columns=feature_columns,\n",
    "      hidden_units=[2560, 1024, 256],\n",
    "      model_dir=model_dir,\n",
    "      config=config,\n",
    "      n_classes=2)\n",
    "  logging.info('training and evaluating estimator model')\n",
    "  # Need to specify both max_steps and epochs. Each worker will go through epoch separately.\n",
    "  # see https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate?version=stable\n",
    "  tf.estimator.train_and_evaluate(\n",
    "      estimator,\n",
    "      train_spec=tf.estimator.TrainSpec(input_fn=lambda: get_dataset('train').repeat(EPOCHS), max_steps=get_max_steps()),\n",
    "      eval_spec=tf.estimator.EvalSpec(input_fn=lambda: get_dataset('test')))\n",
    "  logging.info('done evaluating estimator model')\n",
    "\n",
    "def train_estimator_wide_and_deep(strategy, model_dir):\n",
    "  logging.info('training for {} steps'.format(get_max_steps()))\n",
    "  config = tf.estimator.RunConfig(\n",
    "          train_distribute=strategy,\n",
    "          eval_distribute=strategy)\n",
    "  linear_feature_columns=create_linear_feature_columns()\n",
    "  categorical_feature_columns=create_categorical_feature_columns()\n",
    "  estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "      dnn_optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      linear_optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      linear_feature_columns=linear_feature_columns,\n",
    "      dnn_feature_columns=linear_feature_columns + categorical_feature_columns,\n",
    "      dnn_hidden_units=[2560, 1024, 256],\n",
    "      model_dir=model_dir,\n",
    "      config=config,\n",
    "      n_classes=2)\n",
    "  logging.info('training wide and deep estimator model')\n",
    "  # Need to specify both max_steps and epochs. Each worker will go through epoch separately.\n",
    "  # see https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate?version=stable\n",
    "  tf.estimator.train_and_evaluate(\n",
    "      estimator,\n",
    "      train_spec=tf.estimator.TrainSpec(input_fn=lambda: get_dataset('train').repeat(EPOCHS), max_steps=get_max_steps()),\n",
    "      eval_spec=tf.estimator.EvalSpec(input_fn=lambda: get_dataset('test')))\n",
    "  logging.info('done evaluating wide and deep estimator model')\n",
    "\n",
    "def train_custom_loop_keras_sequential(strategy, model_dir):\n",
    "  train_custom_loop(strategy, create_keras_model_sequential(), model_dir)\n",
    "\n",
    "def train_custom_loop_keras_model_functional_no_feature_layer(strategy, model_dir):\n",
    "  train_custom_loop(strategy, create_keras_model_functional_no_feature_layer(), model_dir)\n",
    "\n",
    "def train_custom_loop(strategy, model, model_dir):\n",
    "  logging.info('training using custom loop')\n",
    "\n",
    "  log_dir= os.path.join(model_dir, \"logs\")\n",
    "  if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "  file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "  file_writer.set_as_default()\n",
    "  # This is fine for MirroredStrategy and TPUStrategy, but has to be changed to run on chief only\n",
    "  # once multi-node training is supported (ParameterServer, MultiWorkerMirroredStrategy).\n",
    "  batch_summary_callback = BatchAccuracyAndLossSummaryCallback(DATASET_SIZE)\n",
    "  train_time_callback = TrainTimeCallback()\n",
    "  checkpoints_dir= os.path.join(model_dir, \"checkpoints\")\n",
    "  if not os.path.exists(checkpoints_dir):\n",
    "      os.makedirs(checkpoints_dir)\n",
    "\n",
    "  checkpoints_file_path = checkpoints_dir + \"/epochs:{epoch:03d}.hdf5\"\n",
    "\n",
    "  loss_object = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "  loss_fn = lambda labels, predictions: tf.reduce_mean(loss_object(labels, predictions))\n",
    "\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "  train_accuracy = tf.keras.metrics.BinaryAccuracy(\n",
    "      name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.BinaryAccuracy(\n",
    "      name='test_accuracy')\n",
    "  optimizer = tf.optimizers.SGD(learning_rate=0.05)\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(examples, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(examples, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    loss = loss_fn(labels, predictions)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "    test_loss.update_state(loss)\n",
    "    return loss\n",
    "\n",
    "  train_dist_dataset = get_dataset('train')\n",
    "  train_time_callback.on_train_begin()\n",
    "  for epoch in range(EPOCHS):\n",
    "    batch_summary_callback.on_epoch_begin(epoch, {})\n",
    "    train_time_callback.on_epoch_begin(epoch, {})\n",
    "\n",
    "    # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for (examples, labels) in train_dist_dataset:\n",
    "      # batch losses from all replicas\n",
    "      batch_loss_all_replicas = strategy.experimental_run_v2(train_step, args=(examples, labels))\n",
    "      # reduced to a single number both across replicas and across the bacth size\n",
    "      loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, batch_loss_all_replicas, axis=None)\n",
    "      total_loss += loss\n",
    "      train_loss = total_loss / num_batches\n",
    "      batch_summary_callback.on_train_batch_end(\n",
    "        num_batches,\n",
    "        {'accuracy': train_accuracy.result(),\n",
    "        'loss': train_loss })\n",
    "      num_batches += 1\n",
    "    checkpoint.save(checkpoints_file_path.format(epoch=epoch))\n",
    "    train_time_callback.params = {'steps' : num_batches}\n",
    "    train_time_callback.on_epoch_end(epoch, {})\n",
    "    logging.info(\"Epoch: {} - Loss: {}, Accuracy: {}\".format(epoch, loss, train_accuracy.result()))\n",
    "  train_time_callback.on_train_end()\n",
    "\n",
    "  test_dist_dataset = get_dataset('test')\n",
    "\n",
    "  # Keras evaluation\n",
    "  # logging.info(\"done training keras model, evaluating model\")\n",
    "  # verbosity = 1 if TRAIN_LOCATION == TRAIN_LOCATION_TYPE.local else 2\n",
    "  # loss, accuracy = model.evaluate(test_dist_dataset, verbose=verbosity)\n",
    "  # logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "\n",
    "  # TEST LOOP\n",
    "  for (examples, labels) in test_dist_dataset:\n",
    "    batch_loss_all_replicas = strategy.experimental_run_v2(test_step, args=(examples, labels))\n",
    "\n",
    "  logging.info(\"Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}\".format(\n",
    "    train_loss,\n",
    "    train_accuracy.result(),\n",
    "    test_loss.result(),\n",
    "    test_accuracy.result()))\n",
    "\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Define the task arguments with the default values.\n",
    "    Returns:\n",
    "        experiment parameters\n",
    "    \"\"\"\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument(\n",
    "        '--train-location',\n",
    "        help='where to train model - locally or in the cloud',\n",
    "        choices=TRAIN_LOCATION_TYPE_VALUES.split(' '),\n",
    "        default='local')\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--model-name',\n",
    "        help='model name, not used.')\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='folder or GCS location to write checkpoints and export models.',\n",
    "        required=True)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--distribution-strategy',\n",
    "        help='Distribution strategy to use.',\n",
    "        choices=DISTRIBUTION_STRATEGY_TYPE_VALUES.split(' '))\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--training-function',\n",
    "        help='Training function.',\n",
    "        choices=TRAINING_FUNCTION_VALUES.split(' '),\n",
    "        default='train_keras_sequential')\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--batch-size',\n",
    "        help='Batch size for each training and evaluation step.',\n",
    "        type=int,\n",
    "        default=128)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--dataset-size',\n",
    "        help='Size of training set',\n",
    "        choices=['full', 'small'],\n",
    "        default='small')\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--dataset-source',\n",
    "        help='Dataset source.',\n",
    "        choices=['bq', 'gcs'],\n",
    "        default='bq')\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help='Maximum number of training data epochs on which to train.',\n",
    "        default=2,\n",
    "        type=int)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--embeddings-mode',\n",
    "        help='Embeddings mode.',\n",
    "        choices=EMBEDDINGS_MODE_TYPE_VALUES,\n",
    "        default='hashbucket')\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--tensorboard',\n",
    "        action='store_true',\n",
    "        help='Ignored by this script.',\n",
    "        default=False)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--ai-platform-mode',\n",
    "        help='Ignored by this script.',\n",
    "        default=None)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--image-name',\n",
    "        help='Ignored by this script.',\n",
    "        default=None)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--no-gpu',\n",
    "        action='store_true',\n",
    "        help='Disabling GPUs - forces training to happen on CPU.',\n",
    "        default=False)\n",
    "\n",
    "    return args_parser.parse_args()\n",
    "\n",
    "def setup_environment(args):\n",
    "  global TRAIN_LOCATION\n",
    "  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "  os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "\n",
    "  TF_CONFIG = os.environ.get('TF_CONFIG')\n",
    "  if (TRAIN_LOCATION == TRAIN_LOCATION_TYPE.local):\n",
    "    # see https://stackoverflow.com/questions/58868459/tensorflow-assertionerror-on-fit-method\n",
    "    logging.warning('training locally')\n",
    "    if TF_CONFIG:\n",
    "      logging.warning('removing TF_CONFIG')\n",
    "      os.environ.pop('TF_CONFIG')\n",
    "  else:\n",
    "    logging.warning('training in cloud')\n",
    "    os.system('gsutil cp {}/{} .'.format(GOOGLE_APPLICATION_CREDENTIALS_GCS_BUCKET, GOOGLE_APPLICATION_CREDENTIALS))\n",
    "    os.environ[ 'GOOGLE_APPLICATION_CREDENTIALS'] = os.getcwd() + '/' + GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "  if TF_CONFIG and '\"master\"' in TF_CONFIG and args.distribution_strategy:\n",
    "    # If distribution strategy is not set, don't replace 'master' -> 'chief',\n",
    "    # otherwise system will assume that environment works in distributed setting and\n",
    "    # will expect to be executed in distribution strategy scope.\n",
    "    # See b/147248890 and\n",
    "    # https://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/python/distribute/multi_worker_util.py#L235\n",
    "    # Fixed in TF2.1rc2 https://github.com/tensorflow/tensorflow/commit/0390084145761a1d4da3be2bec8c56a28399db14\n",
    "    logging.warning('TF_CONFIG before modification:' + str(os.environ['TF_CONFIG']))\n",
    "    TF_CONFIG = TF_CONFIG.replace('\"master\"', '\"chief\"')\n",
    "    os.environ['TF_CONFIG'] = TF_CONFIG\n",
    "\n",
    "  if TF_CONFIG:\n",
    "    logging.warning('TF_CONFIG from env:' + str(os.environ['TF_CONFIG']))\n",
    "\n",
    "def main():\n",
    "    global BATCH_SIZE\n",
    "    global EPOCHS\n",
    "    global TRAIN_LOCATION\n",
    "    global DATASET_SOURCE\n",
    "    global DATASET_SIZE\n",
    "    global DISTRIBUTION_STRATEGY_TYPE\n",
    "    global EMBEDDINGS_MODE\n",
    "    args = get_args()\n",
    "\n",
    "    TRAIN_LOCATION = TRAIN_LOCATION_TYPE[args.train_location]\n",
    "    logging.info('train_location: ' + str(TRAIN_LOCATION))\n",
    "\n",
    "    if TRAIN_LOCATION != TRAIN_LOCATION_TYPE.local:\n",
    "      logging_client = google.cloud.logging.Client()\n",
    "      logging_client.setup_logging()\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('>>>>>>>>>>>>>>>>>>> trainer started <<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    logging.info('trainer called with following arguments:')\n",
    "    logging.info(' '.join(sys.argv))\n",
    "\n",
    "    if args.no_gpu:\n",
    "      os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "      logging.info('forcing no GPU')\n",
    "\n",
    "    logging.warning('tf version: ' + tf.version.VERSION)\n",
    "\n",
    "    # Uncomment to see environment variables\n",
    "    # logging.warning(os.system('env'))\n",
    "\n",
    "    # Uncomment this line to see Op device placement\n",
    "    # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    # https://github.com/tensorflow/tensorflow/issues/34568\n",
    "    # https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy\n",
    "    # Currently there is a limitation in MultiWorkerMirroredStrategy where TensorFlow ops need to be created after the instance of strategy is created.\n",
    "    distribution_strategy = None\n",
    "    # TPU won't work on this sample because strings are not supported by TPU, see:\n",
    "    # https://cloud.google.com/tpu/docs/troubleshooting#unsupported_data_type\n",
    "    if args.distribution_strategy == 'tf.distribute.experimental.TPUStrategy':\n",
    "      tpu = None\n",
    "      try:\n",
    "        logging.info('resolving to TPU cluster')\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        logging.info('connecting to TPU cluster')\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "      except ValueError as e:\n",
    "        logging.info('error connecting to TPU cluster: %s', e)\n",
    "        return\n",
    "      logging.info('initializing TPU system')\n",
    "      tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "      distribution_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "      logging.info('training using TPUStrategy, tpu.cluster_spec: %s', tpu.cluster_spec())\n",
    "    elif args.distribution_strategy == 'tf.distribute.OneDeviceStrategy':\n",
    "      if args.no_gpu:\n",
    "        distribution_strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "      else:\n",
    "        distribution_strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    elif args.distribution_strategy:\n",
    "      distribution_strategy = eval(args.distribution_strategy)()\n",
    "\n",
    "    logging.info('tensorflow version: ' + tf.version.VERSION)\n",
    "    logging.info('tensorflow_io version: ' + tf_io.version.VERSION)\n",
    "    logging.info('device_lib.list_local_devices(): ' + str(device_lib.list_local_devices()))\n",
    "\n",
    "    DATASET_SOURCE = DATASET_SOURCE_TYPE[args.dataset_source]\n",
    "    logging.info('dataset_source: ' + str(DATASET_SOURCE))\n",
    "    DATASET_SIZE = DATASET_SIZE_TYPE[args.dataset_size]\n",
    "    logging.info('dataset_size: ' + str(DATASET_SIZE))\n",
    "    DISTRIBUTION_STRATEGY_TYPE = args.distribution_strategy\n",
    "    logging.info('distribution_strategy: ' + str(DISTRIBUTION_STRATEGY_TYPE))\n",
    "\n",
    "    model_dir = args.job_dir\n",
    "    # if TRAIN_LOCATION == TRAIN_LOCATION_TYPE.cloud and os.environ.get('HOSTNAME'):\n",
    "    #   model_dir = os.path.join(model_dir, os.environ.get('HOSTNAME'))\n",
    "    # model_dir = os.path.join(model_dir, args.training_function, 'model.joblib')\n",
    "    logging.info('Model will be saved to \"%s...\"', model_dir)\n",
    "\n",
    "    logging.info('training_function arg: ' + str(args.training_function))\n",
    "    training_function = getattr(sys.modules[__name__], args.training_function)\n",
    "    logging.info('training_function: ' + str(training_function))\n",
    "\n",
    "    EMBEDDINGS_MODE = EMBEDDINGS_MODE_TYPE[args.embeddings_mode]\n",
    "    if args.training_function == 'train_keras_functional_no_feature_layer':\n",
    "      EMBEDDINGS_MODE = EMBEDDINGS_MODE_TYPE.manual\n",
    "    logging.info('embeddings_mode: ' + str(EMBEDDINGS_MODE))\n",
    "\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    EPOCHS = args.num_epochs\n",
    "\n",
    "    setup_environment(args)\n",
    "\n",
    "    if not args.distribution_strategy:\n",
    "      logging.info('no distribution_strategy')\n",
    "      training_function(None, model_dir)\n",
    "    else:\n",
    "      if 'estimator' in args.training_function:\n",
    "        logging.info('args.training_function:' + args.training_function)\n",
    "        logging.info('distribution_strategy not in scope: ' + str(type(distribution_strategy)))\n",
    "        training_function(distribution_strategy, model_dir)\n",
    "      else:\n",
    "        with distribution_strategy.scope():\n",
    "          logging.info('distribution_strategy in scope: ' + str(type(distribution_strategy)))\n",
    "          training_function(distribution_strategy, model_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "TRAIN_LOCATION = TRAIN_LOCATION_TYPE.local\n",
    "DATASET_SOURCE = DATASET_SOURCE_TYPE.bq\n",
    "DATASET_SIZE = DATASET_SIZE_TYPE.small\n",
    "DISTRIBUTION_STRATEGY_TYPE = ''\n",
    "EMBEDDINGS_MODE = EMBEDDINGS_MODE_TYPE.hashbucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info('logging is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:training for 8594 steps\n",
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "INFO:root:categorical column cat1 hash_bucket_size 98 dimension 18\n",
      "INFO:root:categorical column cat2 hash_bucket_size 364 dimension 26\n",
      "INFO:root:categorical column cat3 hash_bucket_size 624 dimension 29\n",
      "INFO:root:categorical column cat4 hash_bucket_size 830 dimension 32\n",
      "INFO:root:categorical column cat5 hash_bucket_size 38 dimension 14\n",
      "INFO:root:categorical column cat6 hash_bucket_size 8 - creating indicator column\n",
      "INFO:root:categorical column cat7 hash_bucket_size 1764 dimension 38\n",
      "INFO:root:categorical column cat8 hash_bucket_size 60 dimension 16\n",
      "INFO:root:categorical column cat9 hash_bucket_size 3 - creating indicator column\n",
      "INFO:root:categorical column cat10 hash_bucket_size 1267 dimension 35\n",
      "INFO:root:categorical column cat11 hash_bucket_size 1529 dimension 37\n",
      "INFO:root:categorical column cat12 hash_bucket_size 646 dimension 30\n",
      "INFO:root:categorical column cat13 hash_bucket_size 1356 dimension 36\n",
      "INFO:root:categorical column cat14 hash_bucket_size 23 dimension 13\n",
      "INFO:root:categorical column cat15 hash_bucket_size 1254 dimension 35\n",
      "INFO:root:categorical column cat16 hash_bucket_size 727 dimension 31\n",
      "INFO:root:categorical column cat17 hash_bucket_size 9 - creating indicator column\n",
      "INFO:root:categorical column cat18 hash_bucket_size 836 dimension 32\n",
      "INFO:root:categorical column cat19 hash_bucket_size 284 dimension 24\n",
      "INFO:root:categorical column cat20 hash_bucket_size 4 - creating indicator column\n",
      "INFO:root:categorical column cat21 hash_bucket_size 667 dimension 30\n",
      "INFO:root:categorical column cat22 hash_bucket_size 9 - creating indicator column\n",
      "INFO:root:categorical column cat23 hash_bucket_size 12 dimension 11\n",
      "INFO:root:categorical column cat24 hash_bucket_size 822 dimension 32\n",
      "INFO:root:categorical column cat25 hash_bucket_size 37 dimension 14\n",
      "INFO:root:categorical column cat26 hash_bucket_size 601 dimension 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'models/model8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'models/model8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:root:training wide and deep estimator model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into models/model8/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into models/model8/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7261075, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7261075, step = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 37.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 37.2037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.58624244, step = 100 (2.690 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.58624244, step = 100 (2.690 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.3732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.54430354, step = 200 (2.827 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.54430354, step = 200 (2.827 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.7163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4845165, step = 300 (3.057 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4845165, step = 300 (3.057 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.0441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4885334, step = 400 (3.121 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4885334, step = 400 (3.121 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 24.7272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 24.7272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46846688, step = 500 (4.044 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46846688, step = 500 (4.044 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 56.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 56.8363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5296221, step = 600 (1.759 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5296221, step = 600 (1.759 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 41.5194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 41.5194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5588229, step = 700 (2.408 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5588229, step = 700 (2.408 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 28.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 28.942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5402569, step = 800 (3.455 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5402569, step = 800 (3.455 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 50.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 50.0617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6038813, step = 900 (1.998 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6038813, step = 900 (1.998 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.8572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5581181, step = 1000 (2.954 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5581181, step = 1000 (2.954 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.9924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48179668, step = 1100 (2.778 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48179668, step = 1100 (2.778 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.4969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.4969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5532196, step = 1200 (2.818 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5532196, step = 1200 (2.818 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.6077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.6077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5084091, step = 1300 (3.164 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5084091, step = 1300 (3.164 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44953158, step = 1400 (3.316 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44953158, step = 1400 (3.316 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 40.2919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 40.2919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5545129, step = 1500 (2.482 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5545129, step = 1500 (2.482 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 50.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 50.5704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.51150787, step = 1600 (1.978 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.51150787, step = 1600 (1.978 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.0512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48839334, step = 1700 (3.443 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48839334, step = 1700 (3.443 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 28.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 28.0579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4787153, step = 1800 (3.563 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4787153, step = 1800 (3.563 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.8513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.8513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.50857174, step = 1900 (2.869 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.50857174, step = 1900 (2.869 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 55.3351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 55.3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55096054, step = 2000 (1.807 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55096054, step = 2000 (1.807 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5185602, step = 2100 (2.217 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5185602, step = 2100 (2.217 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.4676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.47024983, step = 2200 (2.988 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.47024983, step = 2200 (2.988 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.4411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.4411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46223673, step = 2300 (3.082 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46223673, step = 2300 (3.082 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44884533, step = 2400 (2.998 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44884533, step = 2400 (2.998 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.6243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4866774, step = 2500 (3.064 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4866774, step = 2500 (3.064 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.7442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.40322948, step = 2600 (2.878 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.40322948, step = 2600 (2.878 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.2511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46592957, step = 2700 (2.759 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46592957, step = 2700 (2.759 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.7749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44716385, step = 2800 (2.874 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44716385, step = 2800 (2.874 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 18.4045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 18.4045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5245747, step = 2900 (5.434 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5245747, step = 2900 (5.434 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 42.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 42.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5081904, step = 3000 (2.353 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5081904, step = 3000 (2.353 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.2322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.2322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.459473, step = 3100 (2.763 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.459473, step = 3100 (2.763 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.1378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46584365, step = 3200 (2.927 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46584365, step = 3200 (2.927 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5316168, step = 3300 (2.940 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5316168, step = 3300 (2.940 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.5535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55003136, step = 3400 (2.980 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55003136, step = 3400 (2.980 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.5248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.5248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.41404969, step = 3500 (2.983 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.41404969, step = 3500 (2.983 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.7126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48427716, step = 3600 (2.966 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48427716, step = 3600 (2.966 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49445745, step = 3700 (2.854 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49445745, step = 3700 (2.854 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4419502, step = 3800 (2.970 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4419502, step = 3800 (2.970 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5007517, step = 3900 (2.961 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5007517, step = 3900 (2.961 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.1604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.1604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.45712233, step = 4000 (3.207 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.45712233, step = 4000 (3.207 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 41.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 41.2861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.47424674, step = 4100 (2.422 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.47424674, step = 4100 (2.422 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49335814, step = 4200 (2.940 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49335814, step = 4200 (2.940 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.54966426, step = 4300 (3.313 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.54966426, step = 4300 (3.313 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 50.5151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 50.5151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5616296, step = 4400 (1.980 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5616296, step = 4400 (1.980 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.4452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.4452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4962024, step = 4500 (2.990 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4962024, step = 4500 (2.990 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.2289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.2289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5644436, step = 4600 (2.760 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5644436, step = 4600 (2.760 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.0309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49213758, step = 4700 (2.855 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49213758, step = 4700 (2.855 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 38.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 38.9847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55293, step = 4800 (2.565 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55293, step = 4800 (2.565 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.6685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.6685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.54674715, step = 4900 (2.884 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.54674715, step = 4900 (2.884 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.7199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5016301, step = 5000 (2.880 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5016301, step = 5000 (2.880 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.3522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46454054, step = 5100 (2.911 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46454054, step = 5100 (2.911 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.6594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.6594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.53822863, step = 5200 (2.971 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.53822863, step = 5200 (2.971 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.1174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.36561, step = 5300 (3.020 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.36561, step = 5300 (3.020 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.4547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.4547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4218511, step = 5400 (2.989 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4218511, step = 5400 (2.989 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.9792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.9792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48248369, step = 5500 (2.943 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48248369, step = 5500 (2.943 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.5317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.29474977, step = 5600 (3.074 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.29474977, step = 5600 (3.074 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 37.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 37.1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4900851, step = 5700 (2.696 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4900851, step = 5700 (2.696 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 18.5647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 18.5647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44015658, step = 5800 (5.387 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44015658, step = 5800 (5.387 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.0116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.52778345, step = 5900 (3.029 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.52778345, step = 5900 (3.029 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.3681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.45832393, step = 6000 (2.998 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.45832393, step = 6000 (2.998 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.5737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55460274, step = 6100 (3.270 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55460274, step = 6100 (3.270 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.4095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46820837, step = 6200 (3.401 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46820837, step = 6200 (3.401 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 55.0661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 55.0661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4421778, step = 6300 (1.816 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4421778, step = 6300 (1.816 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 38.9151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 38.9151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.45900664, step = 6400 (2.571 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.45900664, step = 6400 (2.571 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.9071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.9071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49737635, step = 6500 (3.133 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49737635, step = 6500 (3.133 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 39.0913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 39.0913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5133457, step = 6600 (2.558 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5133457, step = 6600 (2.558 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 28.6986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 28.6986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5564715, step = 6700 (3.484 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5564715, step = 6700 (3.484 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5343137, step = 6800 (2.182 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5343137, step = 6800 (2.182 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.1223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4919926, step = 6900 (2.847 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4919926, step = 6900 (2.847 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.3983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49916214, step = 7000 (2.825 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49916214, step = 7000 (2.825 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.4572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.4572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6133933, step = 7100 (2.902 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6133933, step = 7100 (2.902 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.1593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.1593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.538971, step = 7200 (3.015 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.538971, step = 7200 (3.015 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.3771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 36.3771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55882263, step = 7300 (2.749 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.55882263, step = 7300 (2.749 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.1199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 34.1199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4682141, step = 7400 (2.931 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4682141, step = 7400 (2.931 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.5319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5425265, step = 7500 (3.073 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5425265, step = 7500 (3.073 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.7138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.7138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.42600897, step = 7600 (3.057 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.42600897, step = 7600 (3.057 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.3865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48921737, step = 7700 (3.088 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.48921737, step = 7700 (3.088 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.1448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.51802874, step = 7800 (3.111 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.51802874, step = 7800 (3.111 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.5613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.5613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44314533, step = 7900 (3.070 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.44314533, step = 7900 (3.070 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 32.7478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4283507, step = 8000 (3.054 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4283507, step = 8000 (3.054 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 24.6602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 24.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46359846, step = 8100 (4.055 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.46359846, step = 8100 (4.055 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.4719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.4719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49872124, step = 8200 (3.394 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.49872124, step = 8200 (3.394 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 42.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 42.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4536221, step = 8300 (2.380 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4536221, step = 8300 (2.380 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 40.8398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 40.8398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5048914, step = 8400 (2.449 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5048914, step = 8400 (2.449 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 56.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 56.312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4079446, step = 8500 (1.776 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.4079446, step = 8500 (1.776 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 8594 into models/model8/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 8594 into models/model8/model.ckpt.\n",
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-02-11T16:34:19Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-02-11T16:34:19Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model8/model.ckpt-8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model8/model.ckpt-8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 7.76320s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 7.76320s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-02-11-16:34:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-02-11-16:34:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 8594: accuracy = 0.768125, accuracy_baseline = 0.74859375, auc = 0.7414801, auc_precision_recall = 0.4987863, average_loss = 0.4911972, global_step = 8594, label/mean = 0.25140625, loss = 0.4911972, precision = 0.5941265, prediction/mean = 0.25330567, recall = 0.24518335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 8594: accuracy = 0.768125, accuracy_baseline = 0.74859375, auc = 0.7414801, auc_precision_recall = 0.4987863, average_loss = 0.4911972, global_step = 8594, label/mean = 0.25140625, loss = 0.4911972, precision = 0.5941265, prediction/mean = 0.25330567, recall = 0.24518335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8594: models/model8/model.ckpt-8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8594: models/model8/model.ckpt-8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.51406664.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.51406664.\n",
      "INFO:root:done evaluating wide and deep estimator model\n",
      "INFO:root:total train time: (hh:mm:ss.ms) 0:04:34.260982\n"
     ]
    }
   ],
   "source": [
    "def train_estimator_wide_and_deep(strategy, model_dir):\n",
    "  logging.info('training for {} steps'.format(get_max_steps()))\n",
    "  config = tf.estimator.RunConfig(\n",
    "          train_distribute=strategy,\n",
    "          eval_distribute=strategy)\n",
    "  linear_feature_columns=create_linear_feature_columns()\n",
    "  categorical_feature_columns=create_categorical_feature_columns()\n",
    "  estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "      #dnn_optimizer='Adagrad',\n",
    "      #linear_optimizer='Ftrl',\n",
    "      dnn_optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      linear_optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "\n",
    "      linear_feature_columns=linear_feature_columns,\n",
    "      dnn_feature_columns=linear_feature_columns + categorical_feature_columns,\n",
    "      dnn_hidden_units=[1024, 256],\n",
    "      model_dir=model_dir,\n",
    "      config=config,\n",
    "      n_classes=2)\n",
    "#      dnn_dropout=0.01,\n",
    "#      batch_norm=True)\n",
    "  logging.info('training wide and deep estimator model')\n",
    "  # Need to specify both max_steps and epochs. Each worker will go through epoch separately.\n",
    "  # see https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate?version=stable\n",
    "  tf.estimator.train_and_evaluate(\n",
    "      estimator,\n",
    "      train_spec=tf.estimator.TrainSpec(input_fn=lambda: get_dataset('train').repeat(EPOCHS), max_steps=get_max_steps()),\n",
    "      eval_spec=tf.estimator.EvalSpec(input_fn=lambda: get_dataset('test')))\n",
    "  logging.info('done evaluating wide and deep estimator model')\n",
    "\n",
    "training_start_time = datetime.datetime.now()\n",
    "train_estimator_wide_and_deep(None, \"models/model8\")\n",
    "logging.info('total train time: (hh:mm:ss.ms) {}'.format(datetime.datetime.now() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "def read_bigquery_raw(table_name):\n",
    "  if DATASET_SIZE == DATASET_SIZE_TYPE.small:\n",
    "    table_name += '_small'\n",
    "\n",
    "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "  corpus = get_corpus()\n",
    "  requested_streams_count = 10\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + PROJECT_ID,\n",
    "      PROJECT_ID, table_name, DATASET_ID,\n",
    "      list(field.name for field in CSV_SCHEMA),\n",
    "      list(dtypes.int64 if field.field_type == 'INTEGER'\n",
    "           else dtypes.string for field in CSV_SCHEMA),\n",
    "      requested_streams=requested_streams_count)\n",
    "\n",
    "  # manually sharding output instaead of using return read_session.parallel_read_rows()\n",
    "  streams = read_session.get_streams()\n",
    "  # streams_count = len(streams) # does not work for Estimator\n",
    "  streams_count = tf.size(streams)\n",
    "  streams_count64 = tf.cast(streams_count, dtype=tf.int64)\n",
    "  streams_ds = dataset_ops.Dataset.from_tensor_slices(streams).shuffle(buffer_size=streams_count64)\n",
    "  dataset = streams_ds.interleave(\n",
    "            read_session.read_rows,\n",
    "            cycle_length=streams_count64,\n",
    "            num_parallel_calls=streams_count64)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "ds = read_bigquery_raw('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "(mean_dict, std_dict) = get_mean_and_std_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/alekseyv/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "corpus = get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fc7797d99e0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for row, label in ds.batch(10).map(lambda row : tm(row)).prefetch(2).take(2):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fc7797d99e0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for row, label in ds.batch(10).map(lambda row : tm(row)).prefetch(2).take(2):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fc7797d99e0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for row, label in ds.batch(10).map(lambda row : tm(row)).prefetch(2).take(2):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "Tensor(\"Cast:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_1:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_2:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_3:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_4:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_5:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_6:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_7:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_8:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_9:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_10:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_11:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"Cast_12:0\", shape=(None,), dtype=float32)\n",
      "[-0.0547863692 -0.373503059 -0.267264158 ... -0.373503059 -0.373503059 -0.373503059]\n",
      "[-0.274814516 -0.274814516 -0.148401216 ... -0.277394384 -0.238696426 0.540422261]\n",
      "[-0.0556667633 -0.0670353398 -0.00735031161 ... -0.0755617693 -0.0755617693 -0.0755617693]\n",
      "[0.536014915 -0.721518338 2.82243896 ... -0.83583951 -0.378554732 -0.492875934]\n",
      "[-0.266516685 3.57681775 -0.265390694 ... -0.227539986 -0.0427615158 -0.225432366]\n",
      "[-0.296642959 -0.351506025 -0.250923693 ... 0.282467484 -0.351506025 5.53713274]\n",
      "[-0.173857436 -0.250487566 -0.235161528 ... -0.127879351 -0.250487566 -0.189183459]\n",
      "[0.489200503 -0.682726502 1.72623456 ... 1.72623456 0.0334511138 1.79134154]\n",
      "[-0.172094032 -0.477913439 -0.185586065 ... 0.695893407 0.00779973809 3.62816191]\n",
      "[0.557592154 -0.901198149 0.557592154 ... -0.901198149 -0.901198149 -0.901198149]\n",
      "[-0.140004009 -0.524316967 -0.332160473 ... -0.140004009 -0.524316967 -0.140004009]\n",
      "[-0.186158791 -0.186158791 -0.186158791 ... -0.186158791 -0.186158791 -0.186158791]\n",
      "[0.278619796 -0.525729597 1.74107325 ... -0.598852277 -0.306361586 -0.379484266]\n",
      "[-0.373503059 -0.373503059 -0.373503059 ... -0.373503059 0.157691419 -0.373503059]\n",
      "example tf.Tensor([4 1 1 3 2 1 2 1 4 1], shape=(10,), dtype=int64)\n",
      "[-0.150981084 -0.212897807 -0.269654781 ... -0.274814516 -0.212897807 -0.0374671035]\n",
      "[-0.0727196261 -0.069877483 0.0153868413 ... -0.0641932 -0.0755617693 -0.0585089065]\n",
      "[-0.149912313 -0.378554732 0.307372481 ... -0.721518338 0.878978491 1.10762095]\n",
      "[-0.169103801 0.0688562319 -0.267296225 ... -0.267296225 -0.263124287 -0.226976991]\n",
      "[-0.351506025 -0.351506025 -0.351506025 ... -0.351506025 -0.144245461 2.02894258]\n",
      "[-0.250487566 -0.250487566 -0.250487566 ... -0.250487566 -0.173857436 -0.127879351]\n",
      "[-0.422298282 -0.552512407 0.0334511138 ... -0.422298282 0.0334511138 1.20537806]\n",
      "[-0.419447958 -0.459924042 -0.419447958 ... -0.432939976 -0.307014346 0.277640373]\n",
      "[-0.901198149 -0.901198149 -0.901198149 ... -0.901198149 2.01638246 -0.901198149]\n",
      "[-0.524316967 -0.524316967 -0.524316967 ... -0.524316967 -0.140004009 -0.332160473]\n",
      "[-0.186158791 -0.186158791 -0.186158791 ... -0.186158791 0.56610775 -0.186158791]\n",
      "[-0.16011624 -0.306361586 0.351742476 ... -0.379484266 0.497987807 1.30233717]\n",
      "example tf.Tensor([ 5  2  1 22  1  6  1  1  1  1], shape=(10,), dtype=int64)\n",
      "[-0.373503059 -0.373503059 -0.373503059 ... -0.373503059 -0.373503059 -0.373503059]\n",
      "[-0.230956838 -0.192258894 -0.169040129 ... -0.192258894 -0.218057528 -0.109703273]\n",
      "[-0.0755617693 -0.0755617693 -0.0670353398 ... -0.069877483 -0.0755617693 -0.0755617693]\n",
      "[-0.721518338 0.0787300915 -0.149912313 ... 0.650336087 -0.721518338 -0.83583951]\n",
      "[-0.231091201 -0.15837799 0.150259808 ... -0.0944128707 -0.198148668 -0.225605592]\n",
      "[0.300755173 2.60805297 -0.351506025 ... -0.351506025 -0.351506025 -0.0406151749]\n",
      "[0.745704114 -0.235161528 -0.250487566 ... -0.250487566 -0.250487566 -0.143205374]\n",
      "[0.81473577 -0.552512407 -0.422298282 ... 0.0334511138 -0.74783355 2.05176973]\n",
      "[0.583459795 0.232666939 -0.365479827 ... -0.468918741 -0.414950609 0.799332321]\n",
      "[-0.901198149 -0.901198149 -0.901198149 ... -0.901198149 -0.901198149 -0.901198149]\n",
      "[1.20509124 -0.332160473 -0.524316967 ... -0.524316967 -0.524316967 0.244308919]\n",
      "[-0.186158791 -0.186158791 -0.186158791 ... -0.186158791 -0.186158791 -0.186158791]\n",
      "[-0.525729597 -0.0138708921 -0.16011624 ... 0.351742476 -0.525729597 -0.598852277]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_MODE = EMBEDDINGS_MODE_TYPE.manual\n",
    "\n",
    "@tf.function\n",
    "def tm(example):\n",
    "  dict_without_label = dict(example)\n",
    "  label = dict_without_label.pop('label')\n",
    "  #tf.print(float(dict_without_label['int1']))\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        value = float(dict_without_label[field.name])\n",
    "        print(value)\n",
    "        \n",
    "        tf.print((value - mean_dict[field.name]) / std_dict[field.name])\n",
    "        dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "    elif field.name.startswith('cat'):\n",
    "      if EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.none:\n",
    "        dict_without_label.pop(field.name)\n",
    "      elif EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.manual:\n",
    "        cat = dict_without_label[field.name]\n",
    "        if cat is None:\n",
    "          cat = ''\n",
    "        cat_index = corpus[field.name].lookup(cat)\n",
    "        if cat_index is None:\n",
    "          tf.print('not found for {}'.format(field.name))\n",
    "          cat_index = tf.constant(-1)\n",
    "        dict_without_label[field.name] = cat_index\n",
    "        \n",
    "        #result = tf.cond(value == 0, lambda: value, lambda: value)\n",
    "        #tf.print(result)\n",
    "#        if dict_without_label[field.name] != 0:\n",
    "#            \n",
    "#            dict_without_label[field.name] = value #(value - mean_dict[field.name]) / std_dict[field.name]\n",
    "#        else:\n",
    "#            # use normalized mean value if data is missing\n",
    "#            dict_without_label[field.name] = value # float(mean_dict[field.name] / std_dict[field.name])\n",
    "  return (dict_without_label, label)\n",
    "\n",
    "row_index = 0\n",
    "for row, label in ds.batch(10).map(lambda row : tm(row)).prefetch(2).take(2):\n",
    "  print(\"example %s\" %  row['cat1'])\n",
    "  row_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__tm(example):\n",
      "  do_return = False\n",
      "  retval_ = ag__.UndefinedReturnValue()\n",
      "  with ag__.FunctionScope('tm', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "    dict_without_label = ag__.converted_call(dict, (example,), None, fscope)\n",
      "    label = ag__.converted_call(dict_without_label.pop, ('label',), None, fscope)\n",
      "\n",
      "    def get_state_6():\n",
      "      return ()\n",
      "\n",
      "    def set_state_6(_):\n",
      "      pass\n",
      "\n",
      "    def loop_body(iterates):\n",
      "      field = iterates\n",
      "\n",
      "      def get_state_5():\n",
      "        return dict_without_label[field.name],\n",
      "\n",
      "      def set_state_5(vals):\n",
      "        dict_without_label[field.name], = vals\n",
      "\n",
      "      def if_true_5():\n",
      "        value = ag__.converted_call(float, (dict_without_label[field.name],), None, fscope)\n",
      "        print(value)\n",
      "        ag__.converted_call(tf.print, ((value - mean_dict[field.name]) / std_dict[field.name],), None, fscope)\n",
      "        dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
      "        return ag__.match_staging_level(1, cond_5)\n",
      "\n",
      "      def if_false_5():\n",
      "\n",
      "        def get_state_4():\n",
      "          return dict_without_label[field.name],\n",
      "\n",
      "        def set_state_4(vals):\n",
      "          dict_without_label[field.name], = vals\n",
      "\n",
      "        def if_true_4():\n",
      "\n",
      "          def get_state_3():\n",
      "            return dict_without_label[field.name],\n",
      "\n",
      "          def set_state_3(vals):\n",
      "            dict_without_label[field.name], = vals\n",
      "\n",
      "          def if_true_3():\n",
      "            ag__.converted_call(dict_without_label.pop, (field.name,), None, fscope)\n",
      "            return ag__.match_staging_level(1, cond_3)\n",
      "\n",
      "          def if_false_3():\n",
      "\n",
      "            def get_state_2():\n",
      "              return dict_without_label[field.name],\n",
      "\n",
      "            def set_state_2(vals):\n",
      "              dict_without_label[field.name], = vals\n",
      "\n",
      "            def if_true_2():\n",
      "              cat = dict_without_label[field.name]\n",
      "\n",
      "              def get_state():\n",
      "                return ()\n",
      "\n",
      "              def set_state(_):\n",
      "                pass\n",
      "\n",
      "              def if_true():\n",
      "                cat = ''\n",
      "                return cat\n",
      "\n",
      "              def if_false():\n",
      "                return cat\n",
      "              cond = cat is None\n",
      "              cat = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('cat',), ())\n",
      "              cat_index = ag__.converted_call(corpus[field.name].lookup, (cat,), None, fscope)\n",
      "\n",
      "              def get_state_1():\n",
      "                return ()\n",
      "\n",
      "              def set_state_1(_):\n",
      "                pass\n",
      "\n",
      "              def if_true_1():\n",
      "                ag__.converted_call(tf.print, (ag__.converted_call('not found for {}'.format, (field.name,), None, fscope),), None, fscope)\n",
      "                cat_index = ag__.converted_call(tf.constant, (-1,), None, fscope)\n",
      "                return cat_index\n",
      "\n",
      "              def if_false_1():\n",
      "                return cat_index\n",
      "              cond_1 = cat_index is None\n",
      "              cat_index = ag__.if_stmt(cond_1, if_true_1, if_false_1, get_state_1, set_state_1, ('cat_index',), ())\n",
      "              dict_without_label[field.name] = cat_index\n",
      "              return ag__.match_staging_level(1, cond_2)\n",
      "\n",
      "            def if_false_2():\n",
      "              return ag__.match_staging_level(1, cond_2)\n",
      "            cond_2 = EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.manual\n",
      "            ag__.if_stmt(cond_2, if_true_2, if_false_2, get_state_2, set_state_2, (), ('dict_without_label[field.name]',))\n",
      "            return ag__.match_staging_level(1, cond_3)\n",
      "          cond_3 = EMBEDDINGS_MODE == EMBEDDINGS_MODE_TYPE.none\n",
      "          ag__.if_stmt(cond_3, if_true_3, if_false_3, get_state_3, set_state_3, (), ('dict_without_label[field.name]',))\n",
      "          return ag__.match_staging_level(1, cond_4)\n",
      "\n",
      "        def if_false_4():\n",
      "          return ag__.match_staging_level(1, cond_4)\n",
      "        cond_4 = ag__.converted_call(field.name.startswith, ('cat',), None, fscope)\n",
      "        ag__.if_stmt(cond_4, if_true_4, if_false_4, get_state_4, set_state_4, (), ('dict_without_label[field.name]',))\n",
      "        return ag__.match_staging_level(1, cond_5)\n",
      "      cond_5 = ag__.converted_call(field.name.startswith, ('int',), None, fscope)\n",
      "      ag__.if_stmt(cond_5, if_true_5, if_false_5, get_state_5, set_state_5, (), ('dict_without_label[field.name]',))\n",
      "      return ()\n",
      "    ag__.for_stmt(CSV_SCHEMA, None, loop_body, get_state_6, set_state_6, (), (), ())\n",
      "    do_return = True\n",
      "    retval_ = fscope.mark_return_value((dict_without_label, label))\n",
      "  do_return,\n",
      "  return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.autograph.to_code(tm.python_function))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
