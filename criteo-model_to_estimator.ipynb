{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERAS_TO_ESTIMATOR_FEATURE_NAMES\n",
      "{'int1': 'input_1', 'int2': 'input_2', 'int3': 'input_3', 'int4': 'input_4', 'int5': 'input_5', 'int6': 'input_6', 'int7': 'input_7', 'int8': 'input_8', 'int9': 'input_9', 'int10': 'input_10', 'int11': 'input_11', 'int12': 'input_12', 'int13': 'input_13', 'cat1': 'input_14', 'cat2': 'input_15', 'cat3': 'input_16', 'cat4': 'input_17', 'cat5': 'input_18', 'cat6': 'input_19', 'cat7': 'input_20', 'cat8': 'input_21', 'cat9': 'input_22', 'cat10': 'input_23', 'cat11': 'input_24', 'cat12': 'input_25', 'cat13': 'input_26', 'cat14': 'input_27', 'cat15': 'input_28', 'cat16': 'input_29', 'cat17': 'input_30', 'cat18': 'input_31', 'cat19': 'input_32', 'cat20': 'input_33', 'cat21': 'input_34', 'cat22': 'input_35', 'cat23': 'input_36', 'cat24': 'input_37', 'cat25': 'input_38', 'cat26': 'input_39'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFLAGS = flags.FLAGS\\nflags.DEFINE_string(\"job-dir\", \"\", \"Job directory\")\\n\\n\\ndef main(argv):\\n    if len(argv) < 1:\\n      raise app.UsageError(\"Too few command-line arguments.\")\\n\\n    tf.compat.v1.enable_eager_execution()\\n\\n    model_dir = os.path.join(sys.argv[1], \\'model.joblib\\')\\n    logging.info(\\'Model will be saved to \"%s...\"\\', model_dir)\\n\\n    # #tf.debugging.set_log_device_placement(True)\\n    # print(\"tf.config.experimental.list_logical_devices(GPU): \" + str(tf.config.experimental.list_logical_devices(\\'GPU\\')))\\n    # print(\"tf.config.experimental.list_physical_devices(GPU): \" + str(tf.config.experimental.list_physical_devices(\\'GPU\\')))\\n    # print(\"device_lib.list_local_devices(): \" + str(device_lib.list_local_devices()))\\n    # print(\"tf.test.is_gpu_available(): \" + str(tf.test.is_gpu_available()))\\n\\n    #model = train_keras_model(model_dir)\\n    #evaluate_keras_model(model)\\n\\n    model = create_keras_model()\\n    print(\"model.input_names:\")\\n    print(model._is_graph_network)\\n    print(dir(model))\\n\\n    tf.keras.backend.set_learning_phase(True)\\n    # Define DistributionStrategies and convert the Keras Model to an\\n    # Estimator that utilizes these DistributionStrateges.\\n    # Evaluator is a single worker, so using MirroredStrategy.\\n    # config = tf.estimator.RunConfig(\\n    #         train_distribute=tf.distribute.MirroredStrategy(),\\n    #         eval_distribute=tf.distribute.MirroredStrategy())\\n    # keras_estimator = tf.keras.estimator.model_to_estimator(\\n    #     keras_model=model, config=config, model_dir=model_dir)\\n    keras_estimator = tf.keras.estimator.model_to_estimator(\\n        keras_model=model, model_dir=model_dir)\\n\\n    logging.info(\\'!!!!!!!!!!!!! training MirroredStrategy on keras_estimator !!!!!!!!!!!!!!!!!!!\\')\\n    tf.estimator.train_and_evaluate(\\n        keras_estimator,\\n        train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=5),\\n        eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))\\n\\n\\nif __name__ == \\'__main__\\':\\n  logging_client = google.cloud.logging.Client()\\n  logging_client.setup_logging()\\n  logging.warning(\\'>>>>>>>>>>>>>>>>>>>>>>>>>> app started logging <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\')\\n  print(\\'>>>>>>>>>>>>>. executable <<<<<<<<<<<<<<<<<<<<<<<<\\')\\n  print(sys.executable)\\n  print(sys.version)\\n  print(sys.version_info)\\n  logging.warning(os.system(\\'env\\'))\\n\\n  #print(\\'pip\\')\\n  #print(os.system(\\'pip --version\\'))\\n  #print(os.system(\\'pip list\\'))\\n\\n  os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = GOOGLE_APPLICATION_CREDENTIALS\\n  os.environ[\\'PROJECT_ID\\'] = PROJECT_ID\\n  print(os.system(\\'pwd\\'))\\n  print(os.system(\\'ls -al\\'))\\n  if (os.environ.get(\\'CLOUDSDK_METRICS_COMMAND_NAME\\') == \\'gcloud.ai-platform.local.train\\'):\\n    TARGET = TARGET_TYPE.local\\n    logging.warning(\\'training locally\\')\\n    logging.warning(\\'removing TF_CONFIG\\')\\n    os.environ.pop(\\'TF_CONFIG\\')\\n  else:\\n    TARGET = TARGET_TYPE.cloud\\n    logging.warning(\\'training in cloud\\')\\n    os.system(\\'gsutil cp gs://alekseyv-scalableai-dev-private-bucket/criteo/alekseyv-scalableai-dev-077efe757ef6.json .\\')\\n    os.environ[ \"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getcwd() + \\'/\\' + GOOGLE_APPLICATION_CREDENTIALS\\n    #os.system(\\'gsutil cp gs://alekseyv-scalableai-dev-private-bucket/criteo/tensorflow_io-0.10.0-cp27-cp27mu-manylinux2010_x86_64.wh .\\')\\n    #os.system(\\'pip install --no-deps tensorflow_io-0.10.0-cp27-cp27mu-manylinux2010_x86_64.whl\\')\\n\\n  TF_CONFIG = os.environ.get(\\'TF_CONFIG\\')\\n  if TF_CONFIG and \\'\"master\"\\' in TF_CONFIG:\\n    logging.warning(\\'TF_CONFIG before modification:\\' + str(os.environ[\\'TF_CONFIG\\']))\\n    os.environ[\\'TF_CONFIG\\'] = TF_CONFIG.replace(\\'\"master\"\\', \\'\"chief\"\\')\\n\\n  if TF_CONFIG:\\n    logging.warning(\\'TF_CONFIG:\\' + str(os.environ[\\'TF_CONFIG\\']))\\n  logging.warning(os.system(\\'cat ${GOOGLE_APPLICATION_CREDENTIALS}\\'))\\n  app.run(main)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from tensorflow.python.data.experimental.ops import interleave_ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "import google.cloud.logging\n",
    "\n",
    "LOCATION = 'us'\n",
    "PROJECT_ID = \"alekseyv-scalableai-dev\"\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"alekseyv-scalableai-dev-077efe757ef6.json\"\n",
    "\n",
    "# # Download options.\n",
    "# DATA_URL = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle.tar.gz'\n",
    "\n",
    "# DATASET_ID = 'criteo_kaggle'\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "TARGET_TYPE = Enum('TARGET_TYPE', 'local cloud')\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "      bigquery.SchemaField(\"label\", \"INTEGER\", mode='REQUIRED'),\n",
    "      # using strings because of https://github.com/tensorflow/io/issues/619\n",
    "      bigquery.SchemaField(\"int1\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int2\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int3\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int4\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int5\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int6\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int7\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int8\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int9\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int10\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int11\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int12\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int13\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"cat1\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat2\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat3\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat4\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat5\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat6\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat7\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat8\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat9\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat10\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat11\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat12\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat13\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat14\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat15\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat16\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat17\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat18\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat19\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat20\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat21\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat22\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat23\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat24\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat25\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat26\", \"STRING\")\n",
    "  ]\n",
    "\n",
    "# hack because model_to_estimator does not understand input feature names, see\n",
    "# https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow_estimator/python/estimator/keras.py?rcl=282034610&l=151\n",
    "KERAS_TO_ESTIMATOR_FEATURE_NAMES = {}\n",
    "for i in range(0, len(CSV_SCHEMA)):\n",
    "  if i != 0:  # skip label\n",
    "    KERAS_TO_ESTIMATOR_FEATURE_NAMES[CSV_SCHEMA[i].name] = 'input_{}'.format(i)\n",
    "\n",
    "print('KERAS_TO_ESTIMATOR_FEATURE_NAMES')\n",
    "print(KERAS_TO_ESTIMATOR_FEATURE_NAMES)\n",
    "\n",
    "def get_mean_and_std_dicts():\n",
    "  #client = bigquery.Client(location=\"US\", project=PROJECT_ID)\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  query = \"\"\"\n",
    "    select\n",
    "    AVG(int1) as avg_int1, STDDEV(int1) as std_int1,\n",
    "    AVG(int2) as avg_int2, STDDEV(int2) as std_int2,\n",
    "    AVG(int3) as avg_int3, STDDEV(int3) as std_int3,\n",
    "    AVG(int4) as avg_int4, STDDEV(int4) as std_int4,\n",
    "    AVG(int5) as avg_int5, STDDEV(int5) as std_int5,\n",
    "    AVG(int6) as avg_int6, STDDEV(int6) as std_int6,\n",
    "    AVG(int7) as avg_int7, STDDEV(int7) as std_int7,\n",
    "    AVG(int8) as avg_int8, STDDEV(int8) as std_int8,\n",
    "    AVG(int9) as avg_int9, STDDEV(int9) as std_int9,\n",
    "    AVG(int10) as avg_int10, STDDEV(int10) as std_int10,\n",
    "    AVG(int11) as avg_int11, STDDEV(int11) as std_int11,\n",
    "    AVG(int12) as avg_int12, STDDEV(int12) as std_int12,\n",
    "    AVG(int13) as avg_int13, STDDEV(int13) as std_int13\n",
    "    from `alekseyv-scalableai-dev.criteo_kaggle.days`\n",
    "  \"\"\"\n",
    "  query_job = client.query(\n",
    "      query,\n",
    "      location=\"US\",\n",
    "  )  # API request - starts the query\n",
    "\n",
    "  df = query_job.to_dataframe()\n",
    "  #print(query_job.result())\n",
    "  #print(query_job.errors)\n",
    "  #print(df)\n",
    "\n",
    "  mean_dict = dict((field[0].replace('avg_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('avg'))\n",
    "  std_dict = dict((field[0].replace('std_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('std'))\n",
    "  return (mean_dict, std_dict)\n",
    "\n",
    "def transofrom_row(row_dict, mean_dict, std_dict):\n",
    "  dict_without_label = row_dict.copy()\n",
    "  #tf.print(dict_without_label)\n",
    "  label = dict_without_label.pop('label')\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] == 0:\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        else:\n",
    "            dict_without_label[field.name] = 0.0 # don't use normalized 0 value for nulls\n",
    "\n",
    "  #dict_with_esitmator_keys = { KERAS_TO_ESTIMATOR_FEATURE_NAMES[k]:v for k,v in dict_without_label.items() }\n",
    "  dict_with_esitmator_keys = { k:v for k,v in dict_without_label.items() }\n",
    "\n",
    "  return (dict_with_esitmator_keys, label)\n",
    "\n",
    "def read_bigquery(dataset_id, table_name):\n",
    "\n",
    "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + PROJECT_ID,\n",
    "      PROJECT_ID, table_name, dataset_id,\n",
    "      list(field.name for field in CSV_SCHEMA),\n",
    "      list(dtypes.int64 if field.field_type == 'INTEGER'\n",
    "           else dtypes.string for field in CSV_SCHEMA),\n",
    "      requested_streams=10)\n",
    "\n",
    "  #dataset = read_session.parallel_read_rows()\n",
    "\n",
    "  streams = read_session.get_streams()\n",
    "  tf.print('bq streams: !!!!!!!!!!!!!!!!!!!!!!')\n",
    "  tf.print(streams)\n",
    "  streams_count = 10 # len(streams)\n",
    "  #streams_count = read_session.get_streams().shape\n",
    "  tf.print('big query read session returned {} streams'.format(streams_count))\n",
    "\n",
    "  streams_ds = dataset_ops.Dataset.from_tensor_slices(streams).shuffle(buffer_size=streams_count)\n",
    "  dataset = streams_ds.interleave(\n",
    "            read_session.read_rows,\n",
    "            cycle_length=streams_count,\n",
    "            num_parallel_calls=streams_count)\n",
    "  transformed_ds = dataset.map (lambda row: transofrom_row(row, mean_dict, std_dict), num_parallel_calls=streams_count).prefetch(10000)\n",
    "\n",
    "  # Interleave dataset is not shardable, turning off sharding\n",
    "  # See https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size\n",
    "  # Instead we are shuffling data.\n",
    "  options = tf.data.Options()\n",
    "#  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "  result = transformed_ds.with_options(options)\n",
    "  tf.print(str(result))\n",
    "  return result\n",
    "\n",
    "def get_vocabulary_size_dict():\n",
    "  client = bigquery.Client(location=\"US\", project=PROJECT_ID)\n",
    "  query = \"\"\"\n",
    "    SELECT\n",
    "    COUNT(DISTINCT cat1) as cat1,\n",
    "    COUNT(DISTINCT cat2) as cat2,\n",
    "    COUNT(DISTINCT cat3) as cat3,\n",
    "    COUNT(DISTINCT cat4) as cat4,\n",
    "    COUNT(DISTINCT cat5) as cat5,\n",
    "    COUNT(DISTINCT cat6) as cat6,\n",
    "    COUNT(DISTINCT cat7) as cat7,\n",
    "    COUNT(DISTINCT cat8) as cat8,\n",
    "    COUNT(DISTINCT cat9) as cat9,\n",
    "    COUNT(DISTINCT cat10) as cat10,\n",
    "    COUNT(DISTINCT cat11) as cat11,\n",
    "    COUNT(DISTINCT cat12) as cat12,\n",
    "    COUNT(DISTINCT cat13) as cat13,\n",
    "    COUNT(DISTINCT cat14) as cat14,\n",
    "    COUNT(DISTINCT cat15) as cat15,\n",
    "    COUNT(DISTINCT cat16) as cat16,\n",
    "    COUNT(DISTINCT cat17) as cat17,\n",
    "    COUNT(DISTINCT cat18) as cat18,\n",
    "    COUNT(DISTINCT cat19) as cat19,\n",
    "    COUNT(DISTINCT cat20) as cat20,\n",
    "    COUNT(DISTINCT cat21) as cat21,\n",
    "    COUNT(DISTINCT cat22) as cat22,\n",
    "    COUNT(DISTINCT cat23) as cat23,\n",
    "    COUNT(DISTINCT cat24) as cat24,\n",
    "    COUNT(DISTINCT cat25) as cat25,\n",
    "    COUNT(DISTINCT cat26) as cat26\n",
    "    FROM\n",
    "      `alekseyv-scalableai-dev.criteo_kaggle.days`\n",
    "  \"\"\"\n",
    "  query_job = client.query(\n",
    "      query,\n",
    "      location=\"US\",\n",
    "  )  # API request - starts the query\n",
    "\n",
    "  df = query_job.to_dataframe()\n",
    "  #print(query_job.result())\n",
    "  #print(query_job.errors)\n",
    "  #print(df)\n",
    "  dictionary = dict((field[0], df[field[0]][0]) for field in df.items())\n",
    "  #print(dir(df))\n",
    "  return dictionary\n",
    "\n",
    "def create_categorical_feature_column(categorical_vocabulary_size_dict, key):\n",
    "  hash_bucket_size = min(categorical_vocabulary_size_dict[key], 100000)\n",
    "  # TODO: consider using categorical_column_with_vocabulary_list\n",
    "  categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    #KERAS_TO_ESTIMATOR_FEATURE_NAMES[key],\n",
    "    key,\n",
    "    hash_bucket_size,\n",
    "    dtype=tf.dtypes.string\n",
    "  )\n",
    "  if hash_bucket_size < 10:\n",
    "    return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "  embedding_feature_column = tf.feature_column.embedding_column(\n",
    "      categorical_feature_column,\n",
    "      int(min(50, math.floor(6 * hash_bucket_size**0.25))))\n",
    "  return embedding_feature_column\n",
    "\n",
    "def create_feature_columns(categorical_vocabulary_size_dict):\n",
    "  feature_columns = []\n",
    "  feature_columns.extend(list(tf.feature_column.numeric_column(KERAS_TO_ESTIMATOR_FEATURE_NAMES[field.name], dtype=tf.dtypes.float32)  for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label'))\n",
    "  feature_columns.extend(list(create_categorical_feature_column(categorical_vocabulary_size_dict, key) for key, _ in categorical_vocabulary_size_dict.items()))\n",
    "  return feature_columns\n",
    "\n",
    "def create_keras_model():\n",
    "  categorical_vocabulary_size_dict = get_vocabulary_size_dict()\n",
    "  feature_columns = create_feature_columns(categorical_vocabulary_size_dict)\n",
    "  print(\"categorical_vocabulary_size_dict: \" + str(categorical_vocabulary_size_dict))\n",
    "  feature_layer = tf.keras.layers.DenseFeatures(feature_columns, name=\"feature_layer\")\n",
    "  Dense = tf.keras.layers.Dense\n",
    "  model = tf.keras.Sequential(\n",
    "  [\n",
    "      feature_layer,\n",
    "      Dense(2560, activation=tf.nn.relu),\n",
    "      Dense(1024, activation=tf.nn.relu),\n",
    "      Dense(256, activation=tf.nn.relu),\n",
    "      Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "  # Compile Keras model\n",
    "  model.compile(\n",
    "      # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "      #optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "  # HACK: https://b.corp.google.com/issues/114035274\n",
    "  #model._is_graph_network = True\n",
    "  #model.summary()\n",
    "  return model\n",
    "\n",
    "def train_keras_model(model_dir):\n",
    "  logging.info('training keras model')\n",
    "  #strategy = tf.distribute.experimental.ParameterServerStrategy()\n",
    "  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # doesn't work because of https://b.corp.google.com/issues/142700914\n",
    "  #strategy = tf.distribute.MirroredStrategy()\n",
    "  #strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "  with strategy.scope():\n",
    "    model = create_keras_model()\n",
    "    #training_ds = read_bigquery('criteo_kaggle','days_strings').take(1000000).shuffle(10000).batch(BATCH_SIZE).prefetch(100)\n",
    "    #training_ds = read_bigquery('criteo_kaggle','days').skip(100000).take(50000).shuffle(10000).batch(BATCH_SIZE)\n",
    "    training_ds = read_bigquery('criteo_kaggle','days').take(1000000).shuffle(10000).batch(BATCH_SIZE)\n",
    "    print('checking dataset')\n",
    "\n",
    "    log_dir= model_dir + \"/\" + os.environ['HOSTNAME'] + \"/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, embeddings_freq=1, profile_batch=0)\n",
    "\n",
    "    checkpoints_dir = model_dir + \"/\" + os.environ['HOSTNAME'] + \"/checkpoints\"\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "    checkpoints_file_path = checkpoints_dir + \"/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
    "    # crashing https://github.com/tensorflow/tensorflow/issues/27688\n",
    "    #checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoints_file_path, verbose=1, mode='max')\n",
    "\n",
    "    fit_verbosity = 1 if TARGET == TARGET_TYPE.local else 2\n",
    "    model.fit(training_ds, epochs=2, verbose=fit_verbosity,\n",
    "    callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "  return model\n",
    "\n",
    "def evaluate_keras_model(model):\n",
    "  logging.info('evaluating keras model')\n",
    "  eval_ds = read_bigquery('criteo_kaggle','days').skip(100000).take(50 * BATCH_SIZE).batch(BATCH_SIZE)\n",
    "  loss, accuracy = model.evaluate(eval_ds)\n",
    "  logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "\n",
    "def input_fn():\n",
    "  training_ds = read_bigquery('criteo_kaggle','days').take(1000000).shuffle(10000).batch(BATCH_SIZE)\n",
    "  return training_ds\n",
    "\n",
    "'''\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string(\"job-dir\", \"\", \"Job directory\")\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    if len(argv) < 1:\n",
    "      raise app.UsageError(\"Too few command-line arguments.\")\n",
    "\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "    model_dir = os.path.join(sys.argv[1], 'model.joblib')\n",
    "    logging.info('Model will be saved to \"%s...\"', model_dir)\n",
    "\n",
    "    # #tf.debugging.set_log_device_placement(True)\n",
    "    # print(\"tf.config.experimental.list_logical_devices(GPU): \" + str(tf.config.experimental.list_logical_devices('GPU')))\n",
    "    # print(\"tf.config.experimental.list_physical_devices(GPU): \" + str(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    # print(\"device_lib.list_local_devices(): \" + str(device_lib.list_local_devices()))\n",
    "    # print(\"tf.test.is_gpu_available(): \" + str(tf.test.is_gpu_available()))\n",
    "\n",
    "    #model = train_keras_model(model_dir)\n",
    "    #evaluate_keras_model(model)\n",
    "\n",
    "    model = create_keras_model()\n",
    "    print(\"model.input_names:\")\n",
    "    print(model._is_graph_network)\n",
    "    print(dir(model))\n",
    "\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "    # Define DistributionStrategies and convert the Keras Model to an\n",
    "    # Estimator that utilizes these DistributionStrateges.\n",
    "    # Evaluator is a single worker, so using MirroredStrategy.\n",
    "    # config = tf.estimator.RunConfig(\n",
    "    #         train_distribute=tf.distribute.MirroredStrategy(),\n",
    "    #         eval_distribute=tf.distribute.MirroredStrategy())\n",
    "    # keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "    #     keras_model=model, config=config, model_dir=model_dir)\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model, model_dir=model_dir)\n",
    "\n",
    "    logging.info('!!!!!!!!!!!!! training MirroredStrategy on keras_estimator !!!!!!!!!!!!!!!!!!!')\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        keras_estimator,\n",
    "        train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=5),\n",
    "        eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging_client = google.cloud.logging.Client()\n",
    "  logging_client.setup_logging()\n",
    "  logging.warning('>>>>>>>>>>>>>>>>>>>>>>>>>> app started logging <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "  print('>>>>>>>>>>>>>. executable <<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "  print(sys.executable)\n",
    "  print(sys.version)\n",
    "  print(sys.version_info)\n",
    "  logging.warning(os.system('env'))\n",
    "\n",
    "  #print('pip')\n",
    "  #print(os.system('pip --version'))\n",
    "  #print(os.system('pip list'))\n",
    "\n",
    "  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "  os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "  print(os.system('pwd'))\n",
    "  print(os.system('ls -al'))\n",
    "  if (os.environ.get('CLOUDSDK_METRICS_COMMAND_NAME') == 'gcloud.ai-platform.local.train'):\n",
    "    TARGET = TARGET_TYPE.local\n",
    "    logging.warning('training locally')\n",
    "    logging.warning('removing TF_CONFIG')\n",
    "    os.environ.pop('TF_CONFIG')\n",
    "  else:\n",
    "    TARGET = TARGET_TYPE.cloud\n",
    "    logging.warning('training in cloud')\n",
    "    os.system('gsutil cp gs://alekseyv-scalableai-dev-private-bucket/criteo/alekseyv-scalableai-dev-077efe757ef6.json .')\n",
    "    os.environ[ \"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getcwd() + '/' + GOOGLE_APPLICATION_CREDENTIALS\n",
    "    #os.system('gsutil cp gs://alekseyv-scalableai-dev-private-bucket/criteo/tensorflow_io-0.10.0-cp27-cp27mu-manylinux2010_x86_64.wh .')\n",
    "    #os.system('pip install --no-deps tensorflow_io-0.10.0-cp27-cp27mu-manylinux2010_x86_64.whl')\n",
    "\n",
    "  TF_CONFIG = os.environ.get('TF_CONFIG')\n",
    "  if TF_CONFIG and '\"master\"' in TF_CONFIG:\n",
    "    logging.warning('TF_CONFIG before modification:' + str(os.environ['TF_CONFIG']))\n",
    "    os.environ['TF_CONFIG'] = TF_CONFIG.replace('\"master\"', '\"chief\"')\n",
    "\n",
    "  if TF_CONFIG:\n",
    "    logging.warning('TF_CONFIG:' + str(os.environ['TF_CONFIG']))\n",
    "  logging.warning(os.system('cat ${GOOGLE_APPLICATION_CREDENTIALS}'))\n",
    "  app.run(main)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-50-6106ddd700a9>:20: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-50-6106ddd700a9>:20: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0: OrderedDict([('label', <tf.Tensor: id=82551, shape=(), dtype=int32, numpy=0>), ('int1', <tf.Tensor: id=82538, shape=(), dtype=int32, numpy=0>), ('int2', <tf.Tensor: id=82543, shape=(), dtype=int32, numpy=61>), ('int3', <tf.Tensor: id=82544, shape=(), dtype=int32, numpy=3>), ('int4', <tf.Tensor: id=82545, shape=(), dtype=int32, numpy=0>), ('int5', <tf.Tensor: id=82546, shape=(), dtype=int32, numpy=0>), ('int6', <tf.Tensor: id=82547, shape=(), dtype=int32, numpy=0>), ('int7', <tf.Tensor: id=82548, shape=(), dtype=int32, numpy=0>), ('int8', <tf.Tensor: id=82549, shape=(), dtype=int32, numpy=0>), ('int9', <tf.Tensor: id=82550, shape=(), dtype=int32, numpy=0>), ('int10', <tf.Tensor: id=82539, shape=(), dtype=int32, numpy=0>), ('int11', <tf.Tensor: id=82540, shape=(), dtype=int32, numpy=0>), ('int12', <tf.Tensor: id=82541, shape=(), dtype=int32, numpy=0>), ('int13', <tf.Tensor: id=82542, shape=(), dtype=int32, numpy=0>), ('cat1', <tf.Tensor: id=82512, shape=(), dtype=string, numpy=b'05db9164'>), ('cat2', <tf.Tensor: id=82523, shape=(), dtype=string, numpy=b'38a947a1'>), ('cat3', <tf.Tensor: id=82531, shape=(), dtype=string, numpy=b'2939c39c'>), ('cat4', <tf.Tensor: id=82532, shape=(), dtype=string, numpy=b'6a14f9b9'>), ('cat5', <tf.Tensor: id=82533, shape=(), dtype=string, numpy=b'25c83c98'>), ('cat6', <tf.Tensor: id=82534, shape=(), dtype=string, numpy=b'7e0ccccf'>), ('cat7', <tf.Tensor: id=82535, shape=(), dtype=string, numpy=b'e7698644'>), ('cat8', <tf.Tensor: id=82536, shape=(), dtype=string, numpy=b'5b392875'>), ('cat9', <tf.Tensor: id=82537, shape=(), dtype=string, numpy=b'7cc72ec2'>), ('cat10', <tf.Tensor: id=82513, shape=(), dtype=string, numpy=b'3b08e48b'>), ('cat11', <tf.Tensor: id=82514, shape=(), dtype=string, numpy=b'3ec9c616'>), ('cat12', <tf.Tensor: id=82515, shape=(), dtype=string, numpy=b'67e3413b'>), ('cat13', <tf.Tensor: id=82516, shape=(), dtype=string, numpy=b'b55434a9'>), ('cat14', <tf.Tensor: id=82517, shape=(), dtype=string, numpy=b'1adce6ef'>), ('cat15', <tf.Tensor: id=82518, shape=(), dtype=string, numpy=b'fd16dc03'>), ('cat16', <tf.Tensor: id=82519, shape=(), dtype=string, numpy=b'ac388dc0'>), ('cat17', <tf.Tensor: id=82520, shape=(), dtype=string, numpy=b'2005abd1'>), ('cat18', <tf.Tensor: id=82521, shape=(), dtype=string, numpy=b'2b0916a3'>), ('cat19', <tf.Tensor: id=82522, shape=(), dtype=string, numpy=b''>), ('cat20', <tf.Tensor: id=82524, shape=(), dtype=string, numpy=b''>), ('cat21', <tf.Tensor: id=82525, shape=(), dtype=string, numpy=b'4afd0cac'>), ('cat22', <tf.Tensor: id=82526, shape=(), dtype=string, numpy=b''>), ('cat23', <tf.Tensor: id=82527, shape=(), dtype=string, numpy=b'be7c41b4'>), ('cat24', <tf.Tensor: id=82528, shape=(), dtype=string, numpy=b'b34f3128'>), ('cat25', <tf.Tensor: id=82529, shape=(), dtype=string, numpy=b''>), ('cat26', <tf.Tensor: id=82530, shape=(), dtype=string, numpy=b''>)]) \n",
      "\n",
      "\n",
      "row 1: OrderedDict([('label', <tf.Tensor: id=82591, shape=(), dtype=int32, numpy=0>), ('int1', <tf.Tensor: id=82578, shape=(), dtype=int32, numpy=2>), ('int2', <tf.Tensor: id=82583, shape=(), dtype=int32, numpy=255>), ('int3', <tf.Tensor: id=82584, shape=(), dtype=int32, numpy=0>), ('int4', <tf.Tensor: id=82585, shape=(), dtype=int32, numpy=5>), ('int5', <tf.Tensor: id=82586, shape=(), dtype=int32, numpy=4>), ('int6', <tf.Tensor: id=82587, shape=(), dtype=int32, numpy=9>), ('int7', <tf.Tensor: id=82588, shape=(), dtype=int32, numpy=22>), ('int8', <tf.Tensor: id=82589, shape=(), dtype=int32, numpy=16>), ('int9', <tf.Tensor: id=82590, shape=(), dtype=int32, numpy=456>), ('int10', <tf.Tensor: id=82579, shape=(), dtype=int32, numpy=1>), ('int11', <tf.Tensor: id=82580, shape=(), dtype=int32, numpy=7>), ('int12', <tf.Tensor: id=82581, shape=(), dtype=int32, numpy=0>), ('int13', <tf.Tensor: id=82582, shape=(), dtype=int32, numpy=0>), ('cat1', <tf.Tensor: id=82552, shape=(), dtype=string, numpy=b'05db9164'>), ('cat2', <tf.Tensor: id=82563, shape=(), dtype=string, numpy=b'90081f33'>), ('cat3', <tf.Tensor: id=82571, shape=(), dtype=string, numpy=b'2117d0a9'>), ('cat4', <tf.Tensor: id=82572, shape=(), dtype=string, numpy=b'379a7ea8'>), ('cat5', <tf.Tensor: id=82573, shape=(), dtype=string, numpy=b'25c83c98'>), ('cat6', <tf.Tensor: id=82574, shape=(), dtype=string, numpy=b''>), ('cat7', <tf.Tensor: id=82575, shape=(), dtype=string, numpy=b'f01779eb'>), ('cat8', <tf.Tensor: id=82576, shape=(), dtype=string, numpy=b'5b392875'>), ('cat9', <tf.Tensor: id=82577, shape=(), dtype=string, numpy=b'a73ee510'>), ('cat10', <tf.Tensor: id=82553, shape=(), dtype=string, numpy=b'05bd6522'>), ('cat11', <tf.Tensor: id=82554, shape=(), dtype=string, numpy=b'0f1fa8b8'>), ('cat12', <tf.Tensor: id=82555, shape=(), dtype=string, numpy=b'2da6854b'>), ('cat13', <tf.Tensor: id=82556, shape=(), dtype=string, numpy=b'e4e9ce3a'>), ('cat14', <tf.Tensor: id=82557, shape=(), dtype=string, numpy=b'07d13a8f'>), ('cat15', <tf.Tensor: id=82558, shape=(), dtype=string, numpy=b'db23286c'>), ('cat16', <tf.Tensor: id=82559, shape=(), dtype=string, numpy=b'2a98c117'>), ('cat17', <tf.Tensor: id=82560, shape=(), dtype=string, numpy=b'3486227d'>), ('cat18', <tf.Tensor: id=82561, shape=(), dtype=string, numpy=b'32224310'>), ('cat19', <tf.Tensor: id=82562, shape=(), dtype=string, numpy=b''>), ('cat20', <tf.Tensor: id=82564, shape=(), dtype=string, numpy=b''>), ('cat21', <tf.Tensor: id=82565, shape=(), dtype=string, numpy=b'71f6c847'>), ('cat22', <tf.Tensor: id=82566, shape=(), dtype=string, numpy=b''>), ('cat23', <tf.Tensor: id=82567, shape=(), dtype=string, numpy=b'32c7478e'>), ('cat24', <tf.Tensor: id=82568, shape=(), dtype=string, numpy=b'ae362bdd'>), ('cat25', <tf.Tensor: id=82569, shape=(), dtype=string, numpy=b''>), ('cat26', <tf.Tensor: id=82570, shape=(), dtype=string, numpy=b''>)]) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bq streams: !!!!!!!!!!!!!!!!!!!!!!\n",
      "[\"projects/alekseyv-scalableai-dev/locations/us/streams/EgxzaHhoanpHT2hlcTEaAmlyKAE\"]\n",
      "big query read session returned 10 streams\n",
      "<DatasetV1Adapter shapes: ({cat1: (), cat10: (), cat11: (), cat12: (), cat13: (), cat14: (), cat15: (), cat16: (), cat17: (), cat18: (), cat19: (), cat2: (), cat20: (), cat21: (), cat22: (), cat23: (), cat24: (), cat25: (), cat26: (), cat3: (), cat4: (), cat5: (), cat6: (), cat7: (), cat8: (), cat9: (), int1: (), int10: (), int11: (), int12: (), int13: (), int2: (), int3: (), int4: (), int5: (), int6: (), int7: (), int8: (), int9: ()}, ()), types: ({cat1: tf.string, cat10: tf.string, cat11: tf.string, cat12: tf.string, cat13: tf.string, cat14: tf.string, cat15: tf.string, cat16: tf.string, cat17: tf.string, cat18: tf.string, cat19: tf.string, cat2: tf.string, cat20: tf.string, cat21: tf.string, cat22: tf.string, cat23: tf.string, cat24: tf.string, cat25: tf.string, cat26: tf.string, cat3: tf.string, cat4: tf.string, cat5: tf.string, cat6: tf.string, cat7: tf.string, cat8: tf.string, cat9: tf.string, int1: tf.float32, int10: tf.float32, int11: tf.float32, int12: tf.float32, int13: tf.float32, int2: tf.float32, int3: tf.float32, int4: tf.float32, int5: tf.float32, int6: tf.float32, int7: tf.float32, int8: tf.float32, int9: tf.float32}, tf.int64)>\n",
      "row 0: ({'cat1': <tf.Tensor: id=83022, shape=(), dtype=string, numpy=b'8cf07265'>, 'cat10': <tf.Tensor: id=83023, shape=(), dtype=string, numpy=b'e286f1e6'>, 'cat11': <tf.Tensor: id=83024, shape=(), dtype=string, numpy=b'fd7856c1'>, 'cat12': <tf.Tensor: id=83025, shape=(), dtype=string, numpy=b'7cad8267'>, 'cat13': <tf.Tensor: id=83026, shape=(), dtype=string, numpy=b'6a430a5b'>, 'cat14': <tf.Tensor: id=83027, shape=(), dtype=string, numpy=b'b28479f6'>, 'cat15': <tf.Tensor: id=83028, shape=(), dtype=string, numpy=b'4e8979f6'>, 'cat16': <tf.Tensor: id=83029, shape=(), dtype=string, numpy=b'da76182e'>, 'cat17': <tf.Tensor: id=83030, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=83031, shape=(), dtype=string, numpy=b'002c3270'>, 'cat19': <tf.Tensor: id=83032, shape=(), dtype=string, numpy=b''>, 'cat2': <tf.Tensor: id=83033, shape=(), dtype=string, numpy=b'38a947a1'>, 'cat20': <tf.Tensor: id=83034, shape=(), dtype=string, numpy=b''>, 'cat21': <tf.Tensor: id=83035, shape=(), dtype=string, numpy=b'f492dbbf'>, 'cat22': <tf.Tensor: id=83036, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=83037, shape=(), dtype=string, numpy=b'3a171ecb'>, 'cat24': <tf.Tensor: id=83038, shape=(), dtype=string, numpy=b'ba02f03a'>, 'cat25': <tf.Tensor: id=83039, shape=(), dtype=string, numpy=b''>, 'cat26': <tf.Tensor: id=83040, shape=(), dtype=string, numpy=b''>, 'cat3': <tf.Tensor: id=83041, shape=(), dtype=string, numpy=b'642efc0f'>, 'cat4': <tf.Tensor: id=83042, shape=(), dtype=string, numpy=b'48ee52c9'>, 'cat5': <tf.Tensor: id=83043, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=83044, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=83045, shape=(), dtype=string, numpy=b'c7575e05'>, 'cat8': <tf.Tensor: id=83046, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=83047, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=83048, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=83049, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=83050, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=83051, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=83052, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=83053, shape=(), dtype=float32, numpy=-0.27039546>, 'int3': <tf.Tensor: id=83054, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=83055, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=83056, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=83057, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=83058, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=83059, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=83060, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=83061, shape=(), dtype=int64, numpy=0>) \n",
      "\n",
      "\n",
      "row 1: ({'cat1': <tf.Tensor: id=83062, shape=(), dtype=string, numpy=b'05db9164'>, 'cat10': <tf.Tensor: id=83063, shape=(), dtype=string, numpy=b'f6fd64a2'>, 'cat11': <tf.Tensor: id=83064, shape=(), dtype=string, numpy=b'8b94178b'>, 'cat12': <tf.Tensor: id=83065, shape=(), dtype=string, numpy=b'a79b473e'>, 'cat13': <tf.Tensor: id=83066, shape=(), dtype=string, numpy=b'025225f2'>, 'cat14': <tf.Tensor: id=83067, shape=(), dtype=string, numpy=b'b28479f6'>, 'cat15': <tf.Tensor: id=83068, shape=(), dtype=string, numpy=b'4f047de8'>, 'cat16': <tf.Tensor: id=83069, shape=(), dtype=string, numpy=b'9f3b50db'>, 'cat17': <tf.Tensor: id=83070, shape=(), dtype=string, numpy=b'07c540c4'>, 'cat18': <tf.Tensor: id=83071, shape=(), dtype=string, numpy=b'002c3270'>, 'cat19': <tf.Tensor: id=83072, shape=(), dtype=string, numpy=b''>, 'cat2': <tf.Tensor: id=83073, shape=(), dtype=string, numpy=b'38a947a1'>, 'cat20': <tf.Tensor: id=83074, shape=(), dtype=string, numpy=b''>, 'cat21': <tf.Tensor: id=83075, shape=(), dtype=string, numpy=b'f14a2f09'>, 'cat22': <tf.Tensor: id=83076, shape=(), dtype=string, numpy=b'ad3062eb'>, 'cat23': <tf.Tensor: id=83077, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=83078, shape=(), dtype=string, numpy=b'ba02f03a'>, 'cat25': <tf.Tensor: id=83079, shape=(), dtype=string, numpy=b''>, 'cat26': <tf.Tensor: id=83080, shape=(), dtype=string, numpy=b''>, 'cat3': <tf.Tensor: id=83081, shape=(), dtype=string, numpy=b'78a10995'>, 'cat4': <tf.Tensor: id=83082, shape=(), dtype=string, numpy=b'a5a18f25'>, 'cat5': <tf.Tensor: id=83083, shape=(), dtype=string, numpy=b'4cf72387'>, 'cat6': <tf.Tensor: id=83084, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=83085, shape=(), dtype=string, numpy=b'4b3c7cfe'>, 'cat8': <tf.Tensor: id=83086, shape=(), dtype=string, numpy=b'51d76abe'>, 'cat9': <tf.Tensor: id=83087, shape=(), dtype=string, numpy=b'7cc72ec2'>, 'int1': <tf.Tensor: id=83088, shape=(), dtype=float32, numpy=-0.3714482>, 'int10': <tf.Tensor: id=83089, shape=(), dtype=float32, numpy=-0.9027542>, 'int11': <tf.Tensor: id=83090, shape=(), dtype=float32, numpy=-0.525639>, 'int12': <tf.Tensor: id=83091, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=83092, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=83093, shape=(), dtype=float32, numpy=-0.27039546>, 'int3': <tf.Tensor: id=83094, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=83095, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=83096, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=83097, shape=(), dtype=float32, numpy=-0.30337697>, 'int7': <tf.Tensor: id=83098, shape=(), dtype=float32, numpy=-0.24728523>, 'int8': <tf.Tensor: id=83099, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=83100, shape=(), dtype=float32, numpy=-0.48169753>}, <tf.Tensor: id=83101, shape=(), dtype=int64, numpy=0>) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_gcs(table_name, **kwargs):\n",
    "  gcs_filename_glob = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/{}*'.format(table_name)\n",
    "  selected_columns = list(field.name for field in CSV_SCHEMA)\n",
    "  column_names = selected_columns + ['row_hash']\n",
    "\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      gcs_filename_glob,\n",
    "      batch_size=5,\n",
    "      column_names = column_names,\n",
    "      select_columns = selected_columns,\n",
    "      num_epochs=1,\n",
    "      field_delim='\\t',\n",
    "      header=False,\n",
    "      ignore_errors=False,\n",
    "      **kwargs)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "#dataset = read_gcs('test_small').take(10)\n",
    "dataset = read_gcs('test_small').apply(tf.data.experimental.unbatch()).map(lambda row: row).take(2)\n",
    "row_index = 0\n",
    "for row in dataset:\n",
    "  print(\"row %d: %s \\n\\n\" % (row_index, row))\n",
    "  row_index += 1\n",
    "    \n",
    "#dataset = read_gcs('test_small').take(10)\n",
    "dataset = read_bigquery('criteo_kaggle','test_small').take(2)\n",
    "row_index = 0\n",
    "for row in dataset:\n",
    "  print(\"row %d: %s \\n\\n\" % (row_index, row))\n",
    "  row_index += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function <lambda> at 0x7febd40fb9e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function <lambda> at 0x7febd40fb9e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function <lambda> at 0x7febd40fb9e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-73-3e390697af60>:53 <lambda>\n        .map(lambda row: transofrom_row(row)).take(5)\n\n    TypeError: transofrom_row() missing 2 required positional arguments: 'mean_dict' and 'std_dict'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-3e390697af60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m  \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransofrom_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mrow_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \"\"\"\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3414\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3416\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3417\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3418\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1854\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2687\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   2688\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2689\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-73-3e390697af60>:53 <lambda>\n        .map(lambda row: transofrom_row(row)).take(5)\n\n    TypeError: transofrom_row() missing 2 required positional arguments: 'mean_dict' and 'std_dict'\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def transofrom_row_gcs(row_dict):\n",
    "  dict_without_label = row_dict.copy()\n",
    "  label = dict_without_label.pop('label')\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] == 0:\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (value - 0) / 1\n",
    "        else:\n",
    "            dict_without_label[field.name] = 0.0 # don't use normalized 0 value for nulls\n",
    "    elif (field.name != 'label'):\n",
    "      dict_without_label[field.name] = str(dict_without_label[field.name])\n",
    "\n",
    "  dict_with_esitmator_keys = { k:v for k,v in dict_without_label.items() }\n",
    "  return (dict_with_esitmator_keys, label)\n",
    "\n",
    "def transofrom_row_gcs2(row_dict):\n",
    "  dict_without_label = row_dict.copy()\n",
    "  label = tf.reshape(dict_without_label.pop('label'), [])\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] == 0:\n",
    "            value = float(tf.reshape(dict_without_label[field.name], []))\n",
    "            dict_without_label[field.name] = (value - 0) / 1\n",
    "        else:\n",
    "            dict_without_label[field.name] = 0.0 # don't use normalized 0 value for nulls\n",
    "    elif (field.name != 'label'):\n",
    "      dict_without_label[field.name] = str(tf.reshape(dict_without_label[field.name], []))\n",
    "\n",
    "  dict_with_esitmator_keys = { k:v for k,v in dict_without_label.items() }\n",
    "  return (dict_with_esitmator_keys, label)\n",
    "\n",
    "def read_gcs(table_name, **kwargs):\n",
    "  gcs_filename_glob = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/{}*'.format(table_name)\n",
    "  selected_columns = list(field.name for field in CSV_SCHEMA)\n",
    "  column_names = selected_columns + ['row_hash']\n",
    "\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      gcs_filename_glob,\n",
    "      batch_size=1,\n",
    "      column_names = column_names,\n",
    "      select_columns = selected_columns,\n",
    "      num_epochs=1,\n",
    "      field_delim='\\t',\n",
    "      header=False,\n",
    "      ignore_errors=False,\n",
    "      **kwargs)\n",
    "  return dataset\n",
    "\n",
    "dataset = read_gcs('test_small') \\\n",
    " .apply(tf.data.experimental.unbatch()) \\\n",
    " .map(lambda row: transofrom_row(row)).take(5)\n",
    "\n",
    "row_index = 0\n",
    "for row in dataset:\n",
    "  print(\"row %d: %s \\n\\n\" % (row_index, row))\n",
    "  row_index += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.version' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-a4820b89c1a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#tf.dtypes.float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.version' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.version\n",
    "import tensorflow as tf\n",
    "print(tf.version)\n",
    "#tf.dtypes.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 19.3.1 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bq streams: !!!!!!!!!!!!!!!!!!!!!!\n",
      "[\"projects/alekseyv-scalableai-dev/locations/us/streams/EghhZWh5azlWeBoCaXIoAQ\" \"projects/alekseyv-scalableai-dev/locations/us/streams/CAESCGFlaHlrOVZ4GgJpcigB\" \"projects/alekseyv-scalableai-dev/locations/us/streams/CAISCGFlaHlrOVZ4GgJpcigB\" ... \"projects/alekseyv-scalableai-dev/locations/us/streams/CAcSCGFlaHlrOVZ4GgJpcigB\" \"projects/alekseyv-scalableai-dev/locations/us/streams/CAgSCGFlaHlrOVZ4GgJpcigB\" \"projects/alekseyv-scalableai-dev/locations/us/streams/CAkSCGFlaHlrOVZ4GgJpcigB\"]\n",
      "big query read session returned 10 streams\n",
      "<DatasetV1Adapter shapes: ({cat1: (), cat10: (), cat11: (), cat12: (), cat13: (), cat14: (), cat15: (), cat16: (), cat17: (), cat18: (), cat19: (), cat2: (), cat20: (), cat21: (), cat22: (), cat23: (), cat24: (), cat25: (), cat26: (), cat3: (), cat4: (), cat5: (), cat6: (), cat7: (), cat8: (), cat9: (), int1: (), int10: (), int11: (), int12: (), int13: (), int2: (), int3: (), int4: (), int5: (), int6: (), int7: (), int8: (), int9: ()}, ()), types: ({cat1: tf.string, cat10: tf.string, cat11: tf.string, cat12: tf.string, cat13: tf.string, cat14: tf.string, cat15: tf.string, cat16: tf.string, cat17: tf.string, cat18: tf.string, cat19: tf.string, cat2: tf.string, cat20: tf.string, cat21: tf.string, cat22: tf.string, cat23: tf.string, cat24: tf.string, cat25: tf.string, cat26: tf.string, cat3: tf.string, cat4: tf.string, cat5: tf.string, cat6: tf.string, cat7: tf.string, cat8: tf.string, cat9: tf.string, int1: tf.float32, int10: tf.float32, int11: tf.float32, int12: tf.float32, int13: tf.float32, int2: tf.float32, int3: tf.float32, int4: tf.float32, int5: tf.float32, int6: tf.float32, int7: tf.float32, int8: tf.float32, int9: tf.float32}, tf.int64)>\n",
      ">>>>>> row 0: ({'cat1': <tf.Tensor: id=439, shape=(), dtype=string, numpy=b'87552397'>, 'cat10': <tf.Tensor: id=440, shape=(), dtype=string, numpy=b'baa1d4df'>, 'cat11': <tf.Tensor: id=441, shape=(), dtype=string, numpy=b'61593534'>, 'cat12': <tf.Tensor: id=442, shape=(), dtype=string, numpy=b'3563ab62'>, 'cat13': <tf.Tensor: id=443, shape=(), dtype=string, numpy=b'9259d03d'>, 'cat14': <tf.Tensor: id=444, shape=(), dtype=string, numpy=b'1adce6ef'>, 'cat15': <tf.Tensor: id=445, shape=(), dtype=string, numpy=b'a6bf53df'>, 'cat16': <tf.Tensor: id=446, shape=(), dtype=string, numpy=b'b688c8cc'>, 'cat17': <tf.Tensor: id=447, shape=(), dtype=string, numpy=b'27c07bd6'>, 'cat18': <tf.Tensor: id=448, shape=(), dtype=string, numpy=b'65c9624a'>, 'cat19': <tf.Tensor: id=449, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat2': <tf.Tensor: id=450, shape=(), dtype=string, numpy=b'8f5b4275'>, 'cat20': <tf.Tensor: id=451, shape=(), dtype=string, numpy=b'5840adea'>, 'cat21': <tf.Tensor: id=452, shape=(), dtype=string, numpy=b'2754aaf1'>, 'cat22': <tf.Tensor: id=453, shape=(), dtype=string, numpy=b'c9d4222a'>, 'cat23': <tf.Tensor: id=454, shape=(), dtype=string, numpy=b'423fab69'>, 'cat24': <tf.Tensor: id=455, shape=(), dtype=string, numpy=b'3b183c5c'>, 'cat25': <tf.Tensor: id=456, shape=(), dtype=string, numpy=b'e8b83407'>, 'cat26': <tf.Tensor: id=457, shape=(), dtype=string, numpy=b'adb5d234'>, 'cat3': <tf.Tensor: id=458, shape=(), dtype=string, numpy=b'b009d929'>, 'cat4': <tf.Tensor: id=459, shape=(), dtype=string, numpy=b'c7043c4b'>, 'cat5': <tf.Tensor: id=460, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=461, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=462, shape=(), dtype=string, numpy=b'a49e5f93'>, 'cat8': <tf.Tensor: id=463, shape=(), dtype=string, numpy=b'5b392875'>, 'cat9': <tf.Tensor: id=464, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=465, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=466, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=467, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=468, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=469, shape=(), dtype=float32, numpy=-0.50687736>, 'int2': <tf.Tensor: id=470, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=471, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=472, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=473, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=474, shape=(), dtype=float32, numpy=-0.30337697>, 'int7': <tf.Tensor: id=475, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=476, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=477, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=478, shape=(), dtype=int64, numpy=0>)\n",
      ">>>>>> row 1: ({'cat1': <tf.Tensor: id=479, shape=(), dtype=string, numpy=b'05db9164'>, 'cat10': <tf.Tensor: id=480, shape=(), dtype=string, numpy=b'18139a78'>, 'cat11': <tf.Tensor: id=481, shape=(), dtype=string, numpy=b'ea4adb47'>, 'cat12': <tf.Tensor: id=482, shape=(), dtype=string, numpy=b'3563ab62'>, 'cat13': <tf.Tensor: id=483, shape=(), dtype=string, numpy=b'05781932'>, 'cat14': <tf.Tensor: id=484, shape=(), dtype=string, numpy=b'07d13a8f'>, 'cat15': <tf.Tensor: id=485, shape=(), dtype=string, numpy=b'edb12304'>, 'cat16': <tf.Tensor: id=486, shape=(), dtype=string, numpy=b'b688c8cc'>, 'cat17': <tf.Tensor: id=487, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=488, shape=(), dtype=string, numpy=b'65c9624a'>, 'cat19': <tf.Tensor: id=489, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat2': <tf.Tensor: id=490, shape=(), dtype=string, numpy=b'8f5b4275'>, 'cat20': <tf.Tensor: id=491, shape=(), dtype=string, numpy=b'5840adea'>, 'cat21': <tf.Tensor: id=492, shape=(), dtype=string, numpy=b'2754aaf1'>, 'cat22': <tf.Tensor: id=493, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=494, shape=(), dtype=string, numpy=b'dbb486d7'>, 'cat24': <tf.Tensor: id=495, shape=(), dtype=string, numpy=b'3b183c5c'>, 'cat25': <tf.Tensor: id=496, shape=(), dtype=string, numpy=b'e8b83407'>, 'cat26': <tf.Tensor: id=497, shape=(), dtype=string, numpy=b'a9a5cd4f'>, 'cat3': <tf.Tensor: id=498, shape=(), dtype=string, numpy=b'b009d929'>, 'cat4': <tf.Tensor: id=499, shape=(), dtype=string, numpy=b'c7043c4b'>, 'cat5': <tf.Tensor: id=500, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=501, shape=(), dtype=string, numpy=b'fe6b92e5'>, 'cat7': <tf.Tensor: id=502, shape=(), dtype=string, numpy=b'd2d741ca'>, 'cat8': <tf.Tensor: id=503, shape=(), dtype=string, numpy=b'37e4aa92'>, 'cat9': <tf.Tensor: id=504, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=505, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=506, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=507, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=508, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=509, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=510, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=511, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=512, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=513, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=514, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=515, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=516, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=517, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=518, shape=(), dtype=int64, numpy=1>)\n",
      ">>>>>> row 2: ({'cat1': <tf.Tensor: id=519, shape=(), dtype=string, numpy=b'87552397'>, 'cat10': <tf.Tensor: id=520, shape=(), dtype=string, numpy=b'e851ff7b'>, 'cat11': <tf.Tensor: id=521, shape=(), dtype=string, numpy=b'f25fe7e9'>, 'cat12': <tf.Tensor: id=522, shape=(), dtype=string, numpy=b'74563ec1'>, 'cat13': <tf.Tensor: id=523, shape=(), dtype=string, numpy=b'dd183b4c'>, 'cat14': <tf.Tensor: id=524, shape=(), dtype=string, numpy=b'b28479f6'>, 'cat15': <tf.Tensor: id=525, shape=(), dtype=string, numpy=b'4ce39685'>, 'cat16': <tf.Tensor: id=526, shape=(), dtype=string, numpy=b'0c0ca96d'>, 'cat17': <tf.Tensor: id=527, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=528, shape=(), dtype=string, numpy=b'f68751cd'>, 'cat19': <tf.Tensor: id=529, shape=(), dtype=string, numpy=b''>, 'cat2': <tf.Tensor: id=530, shape=(), dtype=string, numpy=b'fc1fa80d'>, 'cat20': <tf.Tensor: id=531, shape=(), dtype=string, numpy=b''>, 'cat21': <tf.Tensor: id=532, shape=(), dtype=string, numpy=b'5cc807cb'>, 'cat22': <tf.Tensor: id=533, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=534, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=535, shape=(), dtype=string, numpy=b'1793a828'>, 'cat25': <tf.Tensor: id=536, shape=(), dtype=string, numpy=b''>, 'cat26': <tf.Tensor: id=537, shape=(), dtype=string, numpy=b''>, 'cat3': <tf.Tensor: id=538, shape=(), dtype=string, numpy=b'dbfc8ee4'>, 'cat4': <tf.Tensor: id=539, shape=(), dtype=string, numpy=b'45e7b9c6'>, 'cat5': <tf.Tensor: id=540, shape=(), dtype=string, numpy=b'43b19349'>, 'cat6': <tf.Tensor: id=541, shape=(), dtype=string, numpy=b''>, 'cat7': <tf.Tensor: id=542, shape=(), dtype=string, numpy=b'968a6688'>, 'cat8': <tf.Tensor: id=543, shape=(), dtype=string, numpy=b'5b392875'>, 'cat9': <tf.Tensor: id=544, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=545, shape=(), dtype=float32, numpy=-0.3714482>, 'int10': <tf.Tensor: id=546, shape=(), dtype=float32, numpy=-0.9027542>, 'int11': <tf.Tensor: id=547, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=548, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=549, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=550, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=551, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=552, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=553, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=554, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=555, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=556, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=557, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=558, shape=(), dtype=int64, numpy=0>)\n",
      ">>>>>> row 3: ({'cat1': <tf.Tensor: id=559, shape=(), dtype=string, numpy=b'68fd1e64'>, 'cat10': <tf.Tensor: id=560, shape=(), dtype=string, numpy=b'09ff33b3'>, 'cat11': <tf.Tensor: id=561, shape=(), dtype=string, numpy=b'5de52301'>, 'cat12': <tf.Tensor: id=562, shape=(), dtype=string, numpy=b'452f9300'>, 'cat13': <tf.Tensor: id=563, shape=(), dtype=string, numpy=b'510f15b3'>, 'cat14': <tf.Tensor: id=564, shape=(), dtype=string, numpy=b'1adce6ef'>, 'cat15': <tf.Tensor: id=565, shape=(), dtype=string, numpy=b'cf995508'>, 'cat16': <tf.Tensor: id=566, shape=(), dtype=string, numpy=b'7c398f49'>, 'cat17': <tf.Tensor: id=567, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=568, shape=(), dtype=string, numpy=b'7d8c03aa'>, 'cat19': <tf.Tensor: id=569, shape=(), dtype=string, numpy=b'4f7d0ae9'>, 'cat2': <tf.Tensor: id=570, shape=(), dtype=string, numpy=b'9b25e48b'>, 'cat20': <tf.Tensor: id=571, shape=(), dtype=string, numpy=b'a458ea53'>, 'cat21': <tf.Tensor: id=572, shape=(), dtype=string, numpy=b'24e5ac20'>, 'cat22': <tf.Tensor: id=573, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=574, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=575, shape=(), dtype=string, numpy=b'4a7a94fc'>, 'cat25': <tf.Tensor: id=576, shape=(), dtype=string, numpy=b'e8b83407'>, 'cat26': <tf.Tensor: id=577, shape=(), dtype=string, numpy=b'47118668'>, 'cat3': <tf.Tensor: id=578, shape=(), dtype=string, numpy=b'1cdd3e43'>, 'cat4': <tf.Tensor: id=579, shape=(), dtype=string, numpy=b'b3ce0655'>, 'cat5': <tf.Tensor: id=580, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=581, shape=(), dtype=string, numpy=b''>, 'cat7': <tf.Tensor: id=582, shape=(), dtype=string, numpy=b'e773d3a8'>, 'cat8': <tf.Tensor: id=583, shape=(), dtype=string, numpy=b'1f89b562'>, 'cat9': <tf.Tensor: id=584, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=585, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=586, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=587, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=588, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=589, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=590, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=591, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=592, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=593, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=594, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=595, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=596, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=597, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=598, shape=(), dtype=int64, numpy=1>)\n",
      ">>>>>> row 4: ({'cat1': <tf.Tensor: id=599, shape=(), dtype=string, numpy=b'f473b8dc'>, 'cat10': <tf.Tensor: id=600, shape=(), dtype=string, numpy=b'd3d8086d'>, 'cat11': <tf.Tensor: id=601, shape=(), dtype=string, numpy=b'd0069af4'>, 'cat12': <tf.Tensor: id=602, shape=(), dtype=string, numpy=b'6c6c6fff'>, 'cat13': <tf.Tensor: id=603, shape=(), dtype=string, numpy=b'e920b070'>, 'cat14': <tf.Tensor: id=604, shape=(), dtype=string, numpy=b'07d13a8f'>, 'cat15': <tf.Tensor: id=605, shape=(), dtype=string, numpy=b'376a23f2'>, 'cat16': <tf.Tensor: id=606, shape=(), dtype=string, numpy=b'5c0831b0'>, 'cat17': <tf.Tensor: id=607, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=608, shape=(), dtype=string, numpy=b'da507f45'>, 'cat19': <tf.Tensor: id=609, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat2': <tf.Tensor: id=610, shape=(), dtype=string, numpy=b'73a46ff0'>, 'cat20': <tf.Tensor: id=611, shape=(), dtype=string, numpy=b'a458ea53'>, 'cat21': <tf.Tensor: id=612, shape=(), dtype=string, numpy=b'24da1a62'>, 'cat22': <tf.Tensor: id=613, shape=(), dtype=string, numpy=b'ad3062eb'>, 'cat23': <tf.Tensor: id=614, shape=(), dtype=string, numpy=b'55dd3565'>, 'cat24': <tf.Tensor: id=615, shape=(), dtype=string, numpy=b'4ac791f9'>, 'cat25': <tf.Tensor: id=616, shape=(), dtype=string, numpy=b'ea9a246c'>, 'cat26': <tf.Tensor: id=617, shape=(), dtype=string, numpy=b'79beb634'>, 'cat3': <tf.Tensor: id=618, shape=(), dtype=string, numpy=b'71a318e5'>, 'cat4': <tf.Tensor: id=619, shape=(), dtype=string, numpy=b'5714e30c'>, 'cat5': <tf.Tensor: id=620, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=621, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=622, shape=(), dtype=string, numpy=b'2a23a943'>, 'cat8': <tf.Tensor: id=623, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=624, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=625, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=626, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=627, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=628, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=629, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=630, shape=(), dtype=float32, numpy=-0.27039546>, 'int3': <tf.Tensor: id=631, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=632, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=633, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=634, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=635, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=636, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=637, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=638, shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>> row 5: ({'cat1': <tf.Tensor: id=639, shape=(), dtype=string, numpy=b'70d60005'>, 'cat10': <tf.Tensor: id=640, shape=(), dtype=string, numpy=b'0466803a'>, 'cat11': <tf.Tensor: id=641, shape=(), dtype=string, numpy=b'3899cbf0'>, 'cat12': <tf.Tensor: id=642, shape=(), dtype=string, numpy=b'd72a8b65'>, 'cat13': <tf.Tensor: id=643, shape=(), dtype=string, numpy=b'5bdabfc1'>, 'cat14': <tf.Tensor: id=644, shape=(), dtype=string, numpy=b'07d13a8f'>, 'cat15': <tf.Tensor: id=645, shape=(), dtype=string, numpy=b'0507b832'>, 'cat16': <tf.Tensor: id=646, shape=(), dtype=string, numpy=b'4b6488bc'>, 'cat17': <tf.Tensor: id=647, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=648, shape=(), dtype=string, numpy=b'b76fb0de'>, 'cat19': <tf.Tensor: id=649, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat2': <tf.Tensor: id=650, shape=(), dtype=string, numpy=b'3ab4d7f5'>, 'cat20': <tf.Tensor: id=651, shape=(), dtype=string, numpy=b'a458ea53'>, 'cat21': <tf.Tensor: id=652, shape=(), dtype=string, numpy=b'6f393b20'>, 'cat22': <tf.Tensor: id=653, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=654, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=655, shape=(), dtype=string, numpy=b'65e74c52'>, 'cat25': <tf.Tensor: id=656, shape=(), dtype=string, numpy=b'c9f3bea7'>, 'cat26': <tf.Tensor: id=657, shape=(), dtype=string, numpy=b'2f494e85'>, 'cat3': <tf.Tensor: id=658, shape=(), dtype=string, numpy=b'109ddc69'>, 'cat4': <tf.Tensor: id=659, shape=(), dtype=string, numpy=b'73ad28a8'>, 'cat5': <tf.Tensor: id=660, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=661, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=662, shape=(), dtype=string, numpy=b'ff37ced5'>, 'cat8': <tf.Tensor: id=663, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=664, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=665, shape=(), dtype=float32, numpy=-0.3714482>, 'int10': <tf.Tensor: id=666, shape=(), dtype=float32, numpy=-0.9027542>, 'int11': <tf.Tensor: id=667, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=668, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=669, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=670, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=671, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=672, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=673, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=674, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=675, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=676, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=677, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=678, shape=(), dtype=int64, numpy=1>)\n",
      ">>>>>> row 6: ({'cat1': <tf.Tensor: id=679, shape=(), dtype=string, numpy=b'05db9164'>, 'cat10': <tf.Tensor: id=680, shape=(), dtype=string, numpy=b'75d433aa'>, 'cat11': <tf.Tensor: id=681, shape=(), dtype=string, numpy=b'755e4a50'>, 'cat12': <tf.Tensor: id=682, shape=(), dtype=string, numpy=b'f3dbd9b0'>, 'cat13': <tf.Tensor: id=683, shape=(), dtype=string, numpy=b'5978055e'>, 'cat14': <tf.Tensor: id=684, shape=(), dtype=string, numpy=b'b28479f6'>, 'cat15': <tf.Tensor: id=685, shape=(), dtype=string, numpy=b'37663ab0'>, 'cat16': <tf.Tensor: id=686, shape=(), dtype=string, numpy=b'4f02a842'>, 'cat17': <tf.Tensor: id=687, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=688, shape=(), dtype=string, numpy=b'f3644223'>, 'cat19': <tf.Tensor: id=689, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat2': <tf.Tensor: id=690, shape=(), dtype=string, numpy=b'52e9ecfc'>, 'cat20': <tf.Tensor: id=691, shape=(), dtype=string, numpy=b'b1252a9d'>, 'cat21': <tf.Tensor: id=692, shape=(), dtype=string, numpy=b'780bdc55'>, 'cat22': <tf.Tensor: id=693, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=694, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=695, shape=(), dtype=string, numpy=b'2f647dfe'>, 'cat25': <tf.Tensor: id=696, shape=(), dtype=string, numpy=b'f0f449dd'>, 'cat26': <tf.Tensor: id=697, shape=(), dtype=string, numpy=b'58bc84e7'>, 'cat3': <tf.Tensor: id=698, shape=(), dtype=string, numpy=b'a9d15bf1'>, 'cat4': <tf.Tensor: id=699, shape=(), dtype=string, numpy=b'6bb5a9c4'>, 'cat5': <tf.Tensor: id=700, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=701, shape=(), dtype=string, numpy=b''>, 'cat7': <tf.Tensor: id=702, shape=(), dtype=string, numpy=b'1c86e0eb'>, 'cat8': <tf.Tensor: id=703, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=704, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=705, shape=(), dtype=float32, numpy=-0.3714482>, 'int10': <tf.Tensor: id=706, shape=(), dtype=float32, numpy=-0.9027542>, 'int11': <tf.Tensor: id=707, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=708, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=709, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=710, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=711, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=712, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=713, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=714, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=715, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=716, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=717, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=718, shape=(), dtype=int64, numpy=1>)\n",
      ">>>>>> row 7: ({'cat1': <tf.Tensor: id=719, shape=(), dtype=string, numpy=b'05db9164'>, 'cat10': <tf.Tensor: id=720, shape=(), dtype=string, numpy=b'f8a0d88b'>, 'cat11': <tf.Tensor: id=721, shape=(), dtype=string, numpy=b'95eaf7a0'>, 'cat12': <tf.Tensor: id=722, shape=(), dtype=string, numpy=b'b5b8de53'>, 'cat13': <tf.Tensor: id=723, shape=(), dtype=string, numpy=b'ccfd4002'>, 'cat14': <tf.Tensor: id=724, shape=(), dtype=string, numpy=b'07d13a8f'>, 'cat15': <tf.Tensor: id=725, shape=(), dtype=string, numpy=b'4bce7416'>, 'cat16': <tf.Tensor: id=726, shape=(), dtype=string, numpy=b'3f8c9229'>, 'cat17': <tf.Tensor: id=727, shape=(), dtype=string, numpy=b'3486227d'>, 'cat18': <tf.Tensor: id=728, shape=(), dtype=string, numpy=b'c6fa25f8'>, 'cat19': <tf.Tensor: id=729, shape=(), dtype=string, numpy=b''>, 'cat2': <tf.Tensor: id=730, shape=(), dtype=string, numpy=b'2607540a'>, 'cat20': <tf.Tensor: id=731, shape=(), dtype=string, numpy=b''>, 'cat21': <tf.Tensor: id=732, shape=(), dtype=string, numpy=b'98040c74'>, 'cat22': <tf.Tensor: id=733, shape=(), dtype=string, numpy=b'ad3062eb'>, 'cat23': <tf.Tensor: id=734, shape=(), dtype=string, numpy=b'423fab69'>, 'cat24': <tf.Tensor: id=735, shape=(), dtype=string, numpy=b'b34f3128'>, 'cat25': <tf.Tensor: id=736, shape=(), dtype=string, numpy=b''>, 'cat26': <tf.Tensor: id=737, shape=(), dtype=string, numpy=b''>, 'cat3': <tf.Tensor: id=738, shape=(), dtype=string, numpy=b'cd5616e7'>, 'cat4': <tf.Tensor: id=739, shape=(), dtype=string, numpy=b'f922efad'>, 'cat5': <tf.Tensor: id=740, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=741, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=742, shape=(), dtype=string, numpy=b'7cfb492c'>, 'cat8': <tf.Tensor: id=743, shape=(), dtype=string, numpy=b'5b392875'>, 'cat9': <tf.Tensor: id=744, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=745, shape=(), dtype=float32, numpy=-0.3714482>, 'int10': <tf.Tensor: id=746, shape=(), dtype=float32, numpy=-0.9027542>, 'int11': <tf.Tensor: id=747, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=748, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=749, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=750, shape=(), dtype=float32, numpy=-0.27039546>, 'int3': <tf.Tensor: id=751, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=752, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=753, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=754, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=755, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=756, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=757, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=758, shape=(), dtype=int64, numpy=0>)\n",
      ">>>>>> row 8: ({'cat1': <tf.Tensor: id=759, shape=(), dtype=string, numpy=b'05db9164'>, 'cat10': <tf.Tensor: id=760, shape=(), dtype=string, numpy=b'76361f0d'>, 'cat11': <tf.Tensor: id=761, shape=(), dtype=string, numpy=b'7fb8555d'>, 'cat12': <tf.Tensor: id=762, shape=(), dtype=string, numpy=b'49fee879'>, 'cat13': <tf.Tensor: id=763, shape=(), dtype=string, numpy=b'5bfcd826'>, 'cat14': <tf.Tensor: id=764, shape=(), dtype=string, numpy=b'b28479f6'>, 'cat15': <tf.Tensor: id=765, shape=(), dtype=string, numpy=b'936289f3'>, 'cat16': <tf.Tensor: id=766, shape=(), dtype=string, numpy=b'3b87fa92'>, 'cat17': <tf.Tensor: id=767, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=768, shape=(), dtype=string, numpy=b'fb342121'>, 'cat19': <tf.Tensor: id=769, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat2': <tf.Tensor: id=770, shape=(), dtype=string, numpy=b'd97d4ce8'>, 'cat20': <tf.Tensor: id=771, shape=(), dtype=string, numpy=b'5840adea'>, 'cat21': <tf.Tensor: id=772, shape=(), dtype=string, numpy=b'd90f665b'>, 'cat22': <tf.Tensor: id=773, shape=(), dtype=string, numpy=b'ad3062eb'>, 'cat23': <tf.Tensor: id=774, shape=(), dtype=string, numpy=b'3a171ecb'>, 'cat24': <tf.Tensor: id=775, shape=(), dtype=string, numpy=b'6c1cdd05'>, 'cat25': <tf.Tensor: id=776, shape=(), dtype=string, numpy=b'ea9a246c'>, 'cat26': <tf.Tensor: id=777, shape=(), dtype=string, numpy=b'1219b447'>, 'cat3': <tf.Tensor: id=778, shape=(), dtype=string, numpy=b'c725873a'>, 'cat4': <tf.Tensor: id=779, shape=(), dtype=string, numpy=b'd0189e5a'>, 'cat5': <tf.Tensor: id=780, shape=(), dtype=string, numpy=b'43b19349'>, 'cat6': <tf.Tensor: id=781, shape=(), dtype=string, numpy=b'fe6b92e5'>, 'cat7': <tf.Tensor: id=782, shape=(), dtype=string, numpy=b'c3ebd8ef'>, 'cat8': <tf.Tensor: id=783, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=784, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=785, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=786, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=787, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=788, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=789, shape=(), dtype=float32, numpy=0.0>, 'int2': <tf.Tensor: id=790, shape=(), dtype=float32, numpy=-0.27039546>, 'int3': <tf.Tensor: id=791, shape=(), dtype=float32, numpy=0.0>, 'int4': <tf.Tensor: id=792, shape=(), dtype=float32, numpy=0.0>, 'int5': <tf.Tensor: id=793, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=794, shape=(), dtype=float32, numpy=0.0>, 'int7': <tf.Tensor: id=795, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=796, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=797, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=798, shape=(), dtype=int64, numpy=0>)\n",
      ">>>>>> row 9: ({'cat1': <tf.Tensor: id=799, shape=(), dtype=string, numpy=b'05db9164'>, 'cat10': <tf.Tensor: id=800, shape=(), dtype=string, numpy=b'322974dd'>, 'cat11': <tf.Tensor: id=801, shape=(), dtype=string, numpy=b'06d58ceb'>, 'cat12': <tf.Tensor: id=802, shape=(), dtype=string, numpy=b''>, 'cat13': <tf.Tensor: id=803, shape=(), dtype=string, numpy=b'5b2b6068'>, 'cat14': <tf.Tensor: id=804, shape=(), dtype=string, numpy=b'07d13a8f'>, 'cat15': <tf.Tensor: id=805, shape=(), dtype=string, numpy=b'bf94b88d'>, 'cat16': <tf.Tensor: id=806, shape=(), dtype=string, numpy=b''>, 'cat17': <tf.Tensor: id=807, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=808, shape=(), dtype=string, numpy=b'8fc2e6f8'>, 'cat19': <tf.Tensor: id=809, shape=(), dtype=string, numpy=b''>, 'cat2': <tf.Tensor: id=810, shape=(), dtype=string, numpy=b'62e9e9bf'>, 'cat20': <tf.Tensor: id=811, shape=(), dtype=string, numpy=b''>, 'cat21': <tf.Tensor: id=812, shape=(), dtype=string, numpy=b''>, 'cat22': <tf.Tensor: id=813, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=814, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=815, shape=(), dtype=string, numpy=b''>, 'cat25': <tf.Tensor: id=816, shape=(), dtype=string, numpy=b''>, 'cat26': <tf.Tensor: id=817, shape=(), dtype=string, numpy=b''>, 'cat3': <tf.Tensor: id=818, shape=(), dtype=string, numpy=b''>, 'cat4': <tf.Tensor: id=819, shape=(), dtype=string, numpy=b''>, 'cat5': <tf.Tensor: id=820, shape=(), dtype=string, numpy=b'43b19349'>, 'cat6': <tf.Tensor: id=821, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=822, shape=(), dtype=string, numpy=b'7307f77d'>, 'cat8': <tf.Tensor: id=823, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=824, shape=(), dtype=string, numpy=b'a73ee510'>, 'int1': <tf.Tensor: id=825, shape=(), dtype=float32, numpy=0.0>, 'int10': <tf.Tensor: id=826, shape=(), dtype=float32, numpy=0.0>, 'int11': <tf.Tensor: id=827, shape=(), dtype=float32, numpy=0.0>, 'int12': <tf.Tensor: id=828, shape=(), dtype=float32, numpy=-0.1770426>, 'int13': <tf.Tensor: id=829, shape=(), dtype=float32, numpy=-0.50687736>, 'int2': <tf.Tensor: id=830, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=831, shape=(), dtype=float32, numpy=-0.06762536>, 'int4': <tf.Tensor: id=832, shape=(), dtype=float32, numpy=-0.8327634>, 'int5': <tf.Tensor: id=833, shape=(), dtype=float32, numpy=0.0>, 'int6': <tf.Tensor: id=834, shape=(), dtype=float32, numpy=-0.30337697>, 'int7': <tf.Tensor: id=835, shape=(), dtype=float32, numpy=0.0>, 'int8': <tf.Tensor: id=836, shape=(), dtype=float32, numpy=0.0>, 'int9': <tf.Tensor: id=837, shape=(), dtype=float32, numpy=0.0>}, <tf.Tensor: id=838, shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "training_ds = read_bigquery('criteo_kaggle','days').take(20).shuffle(2)\n",
    "row_index = 0\n",
    "for row in training_ds.take(10):\n",
    "    #print(\">>>>>> row['title'] %s\" % row['title'] )\n",
    "    print(\">>>>>> row %d: %s\" % (row_index, row))\n",
    "    row_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "categorical_vocabulary_size_dict = get_vocabulary_size_dict()\n",
    "#feature_columns = create_feature_columns(categorical_vocabulary_size_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_columns: [NumericColumn(key='int1', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int3', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int4', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int5', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int6', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int7', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int8', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int9', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int10', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int11', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int12', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='int13', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat1', hash_bucket_size=1460, dtype=tf.string), dimension=37, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadbe4ad0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat2', hash_bucket_size=583, dtype=tf.string), dimension=29, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3f610>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat3', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3f750>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat4', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3f810>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat5', hash_bucket_size=305, dtype=tf.string), dimension=25, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fa10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat6', hash_bucket_size=23, dtype=tf.string), dimension=13, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fed0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat7', hash_bucket_size=12517, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3f7d0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat8', hash_bucket_size=633, dtype=tf.string), dimension=30, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fd10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), IndicatorColumn(categorical_column=HashedCategoricalColumn(key='cat9', hash_bucket_size=3, dtype=tf.string)), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat10', hash_bucket_size=93145, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fb50>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat11', hash_bucket_size=5683, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3f790>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat12', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fbd0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat13', hash_bucket_size=3194, dtype=tf.string), dimension=45, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fe90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat14', hash_bucket_size=27, dtype=tf.string), dimension=13, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fe50>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat15', hash_bucket_size=14992, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3ff50>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat16', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fb90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat17', hash_bucket_size=10, dtype=tf.string), dimension=10, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3ffd0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat18', hash_bucket_size=5652, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3fe10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat19', hash_bucket_size=2172, dtype=tf.string), dimension=40, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb3ff90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), IndicatorColumn(categorical_column=HashedCategoricalColumn(key='cat20', hash_bucket_size=3, dtype=tf.string)), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat21', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb4d490>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat22', hash_bucket_size=17, dtype=tf.string), dimension=12, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb4d310>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat23', hash_bucket_size=15, dtype=tf.string), dimension=11, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb4d150>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat24', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb4d090>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat25', hash_bucket_size=104, dtype=tf.string), dimension=19, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb4d050>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='cat26', hash_bucket_size=100000, dtype=tf.string), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7febadb4d1d0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]\n",
      "\n",
      "feature_layer_inputs: {'int1': <tf.Tensor 'int1_6:0' shape=(None, 1) dtype=float32>, 'int2': <tf.Tensor 'int2_6:0' shape=(None, 1) dtype=float32>, 'int3': <tf.Tensor 'int3_6:0' shape=(None, 1) dtype=float32>, 'int4': <tf.Tensor 'int4_6:0' shape=(None, 1) dtype=float32>, 'int5': <tf.Tensor 'int5_6:0' shape=(None, 1) dtype=float32>, 'int6': <tf.Tensor 'int6_6:0' shape=(None, 1) dtype=float32>, 'int7': <tf.Tensor 'int7_6:0' shape=(None, 1) dtype=float32>, 'int8': <tf.Tensor 'int8_6:0' shape=(None, 1) dtype=float32>, 'int9': <tf.Tensor 'int9_6:0' shape=(None, 1) dtype=float32>, 'int10': <tf.Tensor 'int10_6:0' shape=(None, 1) dtype=float32>, 'int11': <tf.Tensor 'int11_6:0' shape=(None, 1) dtype=float32>, 'int12': <tf.Tensor 'int12_6:0' shape=(None, 1) dtype=float32>, 'int13': <tf.Tensor 'int13_6:0' shape=(None, 1) dtype=float32>, 'cat1': <tf.Tensor 'cat1_1:0' shape=(None,) dtype=string>, 'cat2': <tf.Tensor 'cat2_1:0' shape=(None,) dtype=string>, 'cat3': <tf.Tensor 'cat3_1:0' shape=(None,) dtype=string>, 'cat4': <tf.Tensor 'cat4_1:0' shape=(None,) dtype=string>, 'cat5': <tf.Tensor 'cat5_1:0' shape=(None,) dtype=string>, 'cat6': <tf.Tensor 'cat6_1:0' shape=(None,) dtype=string>, 'cat7': <tf.Tensor 'cat7_1:0' shape=(None,) dtype=string>, 'cat8': <tf.Tensor 'cat8_1:0' shape=(None,) dtype=string>, 'cat9': <tf.Tensor 'cat9_1:0' shape=(None,) dtype=string>, 'cat10': <tf.Tensor 'cat10_1:0' shape=(None,) dtype=string>, 'cat11': <tf.Tensor 'cat11_1:0' shape=(None,) dtype=string>, 'cat12': <tf.Tensor 'cat12_1:0' shape=(None,) dtype=string>, 'cat13': <tf.Tensor 'cat13_1:0' shape=(None,) dtype=string>, 'cat14': <tf.Tensor 'cat14_1:0' shape=(None,) dtype=string>, 'cat15': <tf.Tensor 'cat15_1:0' shape=(None,) dtype=string>, 'cat16': <tf.Tensor 'cat16_1:0' shape=(None,) dtype=string>, 'cat17': <tf.Tensor 'cat17_1:0' shape=(None,) dtype=string>, 'cat18': <tf.Tensor 'cat18_1:0' shape=(None,) dtype=string>, 'cat19': <tf.Tensor 'cat19_1:0' shape=(None,) dtype=string>, 'cat20': <tf.Tensor 'cat20_1:0' shape=(None,) dtype=string>, 'cat21': <tf.Tensor 'cat21_1:0' shape=(None,) dtype=string>, 'cat22': <tf.Tensor 'cat22_1:0' shape=(None,) dtype=string>, 'cat23': <tf.Tensor 'cat23_1:0' shape=(None,) dtype=string>, 'cat24': <tf.Tensor 'cat24_1:0' shape=(None,) dtype=string>, 'cat25': <tf.Tensor 'cat25_1:0' shape=(None,) dtype=string>, 'cat26': <tf.Tensor 'cat26_1:0' shape=(None,) dtype=string>}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: [<tf.Tensor 'int1_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int2_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int3_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int4_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int5_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int6_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int7_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int8_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int9_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int10_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int11_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int12_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'int13_6:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'cat1_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat2_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat3_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat4_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat5_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat6_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat7_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat8_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat9_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat10_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat11_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat12_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat13_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat14_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat15_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat16_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat17_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat18_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat19_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat20_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat21_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat22_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat23_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat24_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat25_1:0' shape=(None,) dtype=string>, <tf.Tensor 'cat26_1:0' shape=(None,) dtype=string>]\n",
      "outputs: Tensor(\"dense_27/Identity:0\", shape=(None, 1), dtype=float32)\n",
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './trained/criteo_kaggle7/model.joblib', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febac737c90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './trained/criteo_kaggle7/model.joblib', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febac737c90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./trained/criteo_kaggle7/model.joblib/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./trained/criteo_kaggle7/model.joblib/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting from: ./trained/criteo_kaggle7/model.joblib/keras/keras_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting from: ./trained/criteo_kaggle7/model.joblib/keras/keras_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-started 32 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-started 32 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into ./trained/criteo_kaggle7/model.joblib/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into ./trained/criteo_kaggle7/model.joblib/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7062943, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7062943, step = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 20 into ./trained/criteo_kaggle7/model.joblib/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 20 into ./trained/criteo_kaggle7/model.joblib/model.ckpt.\n",
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-12-09T11:36:06Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-12-09T11:36:06Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained/criteo_kaggle7/model.joblib/model.ckpt-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained/criteo_kaggle7/model.joblib/model.ckpt-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-12-09-11:36:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-12-09-11:36:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 20: accuracy = 0.5771875, global_step = 20, loss = 0.67990685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 20: accuracy = 0.5771875, global_step = 20, loss = 0.67990685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: ./trained/criteo_kaggle7/model.joblib/model.ckpt-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: ./trained/criteo_kaggle7/model.joblib/model.ckpt-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.6742929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.6742929.\n"
     ]
    }
   ],
   "source": [
    "def create_input_layer(categorical_vocabulary_size_dict):\n",
    "    numeric_feature_columns = list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32)  for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "    numerical_input_layers = {\n",
    "       feature_column.name: tf.keras.layers.Input(name=feature_column.name, shape=(1,), dtype=tf.float32)\n",
    "       for feature_column in numeric_feature_columns\n",
    "    }\n",
    "    categorical_feature_columns = list(create_categorical_feature_column(categorical_vocabulary_size_dict, key) for key, _ in categorical_vocabulary_size_dict.items())\n",
    "    #print(\"categorical_feature_columns: \" + str(categorical_feature_columns))\n",
    "    categorical_input_layers = {\n",
    "       feature_column.categorical_column.name: tf.keras.layers.Input(name=feature_column.categorical_column.name, shape=(), dtype=tf.string)\n",
    "       for feature_column in categorical_feature_columns\n",
    "    }\n",
    "    #print(\"categorical_input_layers: \" + str(categorical_input_layers))\n",
    "    input_layers = numerical_input_layers.copy()\n",
    "    input_layers.update(categorical_input_layers)\n",
    "\n",
    "    return (input_layers, numeric_feature_columns + categorical_feature_columns)\n",
    "\n",
    "def create_keras_model_functional(categorical_vocabulary_size_dict): \n",
    "    (feature_layer_inputs, feature_columns) = create_input_layer(categorical_vocabulary_size_dict)\n",
    "    #print(input_layers)\n",
    "    #x = tf.keras.layers.DenseFeatures(feature_columns)(input_layers)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"feature_columns: \" + str(feature_columns))\n",
    "    print(\"\")\n",
    "    print(\"feature_layer_inputs: \" + str(feature_layer_inputs))\n",
    "    print(\"\")\n",
    "    \n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    \n",
    "    \n",
    "    x = tf.keras.layers.Dense(2560, activation=tf.nn.relu)(feature_layer_outputs)\n",
    "    x = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation=tf.nn.relu)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "    #output = tf.squeeze(output, -1)\n",
    "    inputs=[v for v in feature_layer_inputs.values()]\n",
    "    \n",
    "    print(\"inputs: \" + str(inputs))\n",
    "    print(\"outputs: \" + str(outputs))\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "      # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "      #optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "    print(\"model: \" + str(model.summary()))    \n",
    "    return model    \n",
    "    \n",
    "\n",
    "#strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # doesn't work because of https://b.corp.google.com/issues/142700914\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "with strategy.scope():    \n",
    "    model = create_keras_model_functional(categorical_vocabulary_size_dict)\n",
    "    model_dir = './trained/criteo_kaggle7/model.joblib'\n",
    "    #model.fit(training_ds)\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model, model_dir=model_dir)\n",
    "    logging.info('!!!!!!!!!!!!! training MirroredStrategy on keras_estimator !!!!!!!!!!!!!!!!!!!')\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        keras_estimator,\n",
    "        train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=20),\n",
    "        eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_feature_column(categorical_vocabulary_size_dict, key):\n",
    "  hash_bucket_size = min(categorical_vocabulary_size_dict[key], 100000)\n",
    "  # TODO: consider using categorical_column_with_vocabulary_list\n",
    "  categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    #KERAS_TO_ESTIMATOR_FEATURE_NAMES[key],\n",
    "\n",
    "    hash_bucket_size,\n",
    "    dtype=tf.dtypes.string\n",
    "  )\n",
    "  if hash_bucket_size < 10:\n",
    "    return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "  embedding_feature_column = tf.feature_column.embedding_column(\n",
    "      categorical_feature_column,\n",
    "      int(min(50, math.floor(6 * hash_bucket_size**0.25))))\n",
    "  return embedding_feature_column\n",
    "\n",
    "def create_feature_columns(categorical_vocabulary_size_dict):\n",
    "  feature_columns = []\n",
    "  feature_columns.extend(list(tf.feature_column.numeric_column(KERAS_TO_ESTIMATOR_FEATURE_NAMES[field.name], dtype=tf.dtypes.float32)  for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label'))\n",
    "  feature_columns.extend(list(create_categorical_feature_column(categorical_vocabulary_size_dict, key) for key, _ in categorical_vocabulary_size_dict.items()))\n",
    "  return feature_columns\n",
    "\n",
    "\n",
    "\n",
    "def create_keras_model_sequential():\n",
    "  categorical_vocabulary_size_dict = get_vocabulary_size_dict()\n",
    "  feature_columns = create_feature_columns(categorical_vocabulary_size_dict)\n",
    "  print(\"categorical_vocabulary_size_dict: \" + str(categorical_vocabulary_size_dict))\n",
    "  model = tf.keras.Sequential(\n",
    "  [\n",
    "      tf.keras.layers.DenseFeatures(feature_columns)\n",
    "      tf.keras.layers.Dense(2560, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "  # Compile Keras model\n",
    "  model.compile(\n",
    "      # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "      #optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "      optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "  # HACK: https://b.corp.google.com/issues/114035274\n",
    "  #model._is_graph_network = True\n",
    "  #model.summary()\n",
    "  return model\n",
    "\n",
    "\n",
    "model = create_keras_model_sequential()\n",
    "\n",
    "model_dir = './trained/criteo_kaggle3/model.joblib'\n",
    "\n",
    "\n",
    "keras_estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model=model, model_dir=model_dir)\n",
    "\n",
    "logging.info('!!!!!!!!!!!!! training MirroredStrategy on keras_estimator !!!!!!!!!!!!!!!!!!!')\n",
    "tf.estimator.train_and_evaluate(\n",
    "    keras_estimator,\n",
    "    train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=2),\n",
    "    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprhp54hnl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprhp54hnl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmprhp54hnl', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7febc4582090>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7febd4738d10>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febd47382d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmprhp54hnl', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7febc4582090>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7febd4738d10>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febd47382d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmprhp54hnl/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmprhp54hnl/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.72376955, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.72376955, step = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 2 into /tmp/tmprhp54hnl/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 2 into /tmp/tmprhp54hnl/model.ckpt.\n",
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-12-10T15:14:55Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-12-10T15:14:55Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmprhp54hnl/model.ckpt-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmprhp54hnl/model.ckpt-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-12-10-15:15:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-12-10-15:15:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 2: accuracy = 0.42476562, accuracy_baseline = 0.58671874, auc = 0.49523786, auc_precision_recall = 0.4172819, average_loss = 0.6997528, global_step = 2, label/mean = 0.41328126, loss = 0.6997528, precision = 0.41308177, prediction/mean = 0.5165568, recall = 0.9311909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 2: accuracy = 0.42476562, accuracy_baseline = 0.58671874, auc = 0.49523786, auc_precision_recall = 0.4172819, average_loss = 0.6997528, global_step = 2, label/mean = 0.41328126, loss = 0.6997528, precision = 0.41308177, prediction/mean = 0.5165568, recall = 0.9311909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2: /tmp/tmprhp54hnl/model.ckpt-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2: /tmp/tmprhp54hnl/model.ckpt-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.7019203.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.7019203.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.42476562,\n",
       "  'accuracy_baseline': 0.58671874,\n",
       "  'auc': 0.49523786,\n",
       "  'auc_precision_recall': 0.4172819,\n",
       "  'average_loss': 0.6997528,\n",
       "  'label/mean': 0.41328126,\n",
       "  'loss': 0.6997528,\n",
       "  'precision': 0.41308177,\n",
       "  'prediction/mean': 0.5165568,\n",
       "  'recall': 0.9311909,\n",
       "  'global_step': 2},\n",
       " [])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = './trained/criteo_kaggle_estimator/model.joblib'\n",
    "\n",
    "def create_categorical_feature_column(categorical_vocabulary_size_dict, key):\n",
    "  hash_bucket_size = min(categorical_vocabulary_size_dict[key], 100000)\n",
    "  # TODO: consider using categorical_column_with_vocabulary_list\n",
    "  categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    #KERAS_TO_ESTIMATOR_FEATURE_NAMES[key],\n",
    "    key,\n",
    "    hash_bucket_size,\n",
    "    dtype=tf.dtypes.string\n",
    "  )\n",
    "  if hash_bucket_size < 10:\n",
    "    return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "  embedding_feature_column = tf.feature_column.embedding_column(\n",
    "      categorical_feature_column,\n",
    "      int(min(50, math.floor(6 * hash_bucket_size**0.25))))\n",
    "  return embedding_feature_column\n",
    "\n",
    "def create_feature_columns(categorical_vocabulary_size_dict):\n",
    "  feature_columns = []\n",
    "  feature_columns.extend(list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32)  for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label'))\n",
    "  feature_columns.extend(list(create_categorical_feature_column(categorical_vocabulary_size_dict, key) for key, _ in categorical_vocabulary_size_dict.items()))\n",
    "  return feature_columns\n",
    "\n",
    "categorical_vocabulary_size_dict = get_vocabulary_size_dict()\n",
    "feature_columns = create_feature_columns(categorical_vocabulary_size_dict)\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "        train_distribute=tf.distribute.MirroredStrategy(),\n",
    "        eval_distribute=tf.distribute.MirroredStrategy())\n",
    "config = tf.estimator.RunConfig(\n",
    "        train_distribute=tf.distribute.OneDeviceStrategy(device=\"/cpu:0\"),\n",
    "        eval_distribute=tf.distribute.OneDeviceStrategy(device=\"/cpu:0\"))\n",
    "\n",
    "#model_dir=model_dir,\n",
    "estimator = tf.estimator.DNNClassifier(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[2560, 1024, 256],\n",
    "    \n",
    "    n_classes=2,\n",
    "    config=config)\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator,\n",
    "    train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=2),\n",
    "    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
