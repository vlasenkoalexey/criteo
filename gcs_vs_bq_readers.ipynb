{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from tensorflow.python.data.experimental.ops import interleave_ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "import google.cloud.logging\n",
    "\n",
    "import argparse\n",
    "\n",
    "LOCATION = 'us'\n",
    "PROJECT_ID = \"alekseyv-scalableai-dev\"\n",
    "\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "      bigquery.SchemaField(\"label\", \"INTEGER\", mode='REQUIRED'),\n",
    "      bigquery.SchemaField(\"int1\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int2\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int3\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int4\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int5\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int6\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int7\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int8\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int9\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int10\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int11\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int12\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"int13\", \"INTEGER\"),\n",
    "      bigquery.SchemaField(\"cat1\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat2\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat3\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat4\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat5\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat6\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat7\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat8\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat9\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat10\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat11\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat12\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat13\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat14\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat15\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat16\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat17\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat18\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat19\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat20\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat21\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat22\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat23\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat24\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat25\", \"STRING\"),\n",
    "      bigquery.SchemaField(\"cat26\", \"STRING\")\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'int1': 3.502413317075398, 'int2': 105.8484197976657, 'int3': 26.913041020611118, 'int4': 7.322680248873331, 'int5': 18538.99166487135, 'int6': 116.06185085211608, 'int7': 16.333130032135006, 'int8': 12.51704213755677, 'int9': 106.10982343805144, 'int10': 0.6175294977722182, 'int11': 2.7328343170173177, 'int12': 0.991035628772124, 'int13': 8.217461161174013}\n",
      "{'int1': 9.429076407105086, 'int2': 391.4578226870704, 'int3': 397.97258302273474, 'int4': 8.793230712645805, 'int5': 69394.60184622335, 'int6': 382.5664493712364, 'int7': 66.04975524511708, 'int8': 16.688884567787582, 'int9': 220.28309398647906, 'int10': 0.6840505553977025, 'int11': 5.199070884811354, 'int12': 5.597723872237179, 'int13': 16.21193255817379}\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std_dicts():\n",
    "  #client = bigquery.Client(location=\"US\", project=PROJECT_ID)\n",
    "  client = bigquery.Client(project=PROJECT_ID)\n",
    "  query = \"\"\"\n",
    "    select\n",
    "    AVG(int1) as avg_int1, STDDEV(int1) as std_int1,\n",
    "    AVG(int2) as avg_int2, STDDEV(int2) as std_int2,\n",
    "    AVG(int3) as avg_int3, STDDEV(int3) as std_int3,\n",
    "    AVG(int4) as avg_int4, STDDEV(int4) as std_int4,\n",
    "    AVG(int5) as avg_int5, STDDEV(int5) as std_int5,\n",
    "    AVG(int6) as avg_int6, STDDEV(int6) as std_int6,\n",
    "    AVG(int7) as avg_int7, STDDEV(int7) as std_int7,\n",
    "    AVG(int8) as avg_int8, STDDEV(int8) as std_int8,\n",
    "    AVG(int9) as avg_int9, STDDEV(int9) as std_int9,\n",
    "    AVG(int10) as avg_int10, STDDEV(int10) as std_int10,\n",
    "    AVG(int11) as avg_int11, STDDEV(int11) as std_int11,\n",
    "    AVG(int12) as avg_int12, STDDEV(int12) as std_int12,\n",
    "    AVG(int13) as avg_int13, STDDEV(int13) as std_int13\n",
    "    from `alekseyv-scalableai-dev.criteo_kaggle.days`\n",
    "  \"\"\"\n",
    "  query_job = client.query(\n",
    "      query,\n",
    "      location=\"US\",\n",
    "  )  # API request - starts the query\n",
    "\n",
    "  df = query_job.to_dataframe()\n",
    "  #print(query_job.result())\n",
    "  #print(query_job.errors)\n",
    "  #print(df)\n",
    "\n",
    "  mean_dict = dict((field[0].replace('avg_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('avg'))\n",
    "  std_dict = dict((field[0].replace('std_', ''), df[field[0]][0]) for field in df.items() if field[0].startswith('std'))\n",
    "  return (mean_dict, std_dict)\n",
    "\n",
    "(mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "\n",
    "print(mean_dict)\n",
    "print(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-c5091cb2fe50>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-c5091cb2fe50>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    return (dict_with_esitmator_keys, label)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def transofrom_row(row_dict, mean_dict, std_dict):\n",
    "  tf.print(OrderedDict(row_dict))\n",
    "  dict_without_label = row_dict.copy()\n",
    "  #tf.print(dict_without_label)\n",
    "  label = dict_without_label.pop('label')\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] == 0:\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        else:\n",
    "            dict_without_label[field.name] = 0.0 # don't use normalized 0 value for nulls\n",
    "\n",
    "  tf.print(OrderedDict(dict_without_label)\n",
    "  return (dict_without_label, label)\n",
    "\n",
    "def read_bigquery(dataset_id, table_name):\n",
    "\n",
    "\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + PROJECT_ID,\n",
    "      PROJECT_ID, table_name, dataset_id,\n",
    "      list(field.name for field in CSV_SCHEMA),\n",
    "      list(dtypes.int64 if field.field_type == 'INTEGER'\n",
    "           else dtypes.string for field in CSV_SCHEMA),\n",
    "      requested_streams=10)\n",
    "\n",
    "  #dataset = read_session.parallel_read_rows()\n",
    "\n",
    "  streams = read_session.get_streams()\n",
    "  tf.print('bq streams: !!!!!!!!!!!!!!!!!!!!!!')\n",
    "  tf.print(streams)\n",
    "  streams_count = 10 # len(streams)\n",
    "  #streams_count = read_session.get_streams().shape\n",
    "  tf.print('big query read session returned {} streams'.format(streams_count))\n",
    "\n",
    "  streams_ds = dataset_ops.Dataset.from_tensor_slices(streams).shuffle(buffer_size=streams_count)\n",
    "  dataset = streams_ds.interleave(\n",
    "            read_session.read_rows,\n",
    "            cycle_length=streams_count,\n",
    "            num_parallel_calls=streams_count)\n",
    "  transformed_ds = dataset.map (lambda row: transofrom_row(row, mean_dict, std_dict), num_parallel_calls=streams_count)\n",
    "\n",
    "  # Interleave dataset is not shardable, turning off sharding\n",
    "  # See https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size\n",
    "  # Instead we are shuffling data.\n",
    "  options = tf.data.Options()\n",
    "#  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "  result = transformed_ds.with_options(options)\n",
    "  tf.print(str(result))\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'int1', 'int2', 'int3', 'int4', 'int5', 'int6', 'int7', 'int8', 'int9', 'int10', 'int11', 'int12', 'int13', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'row_hash']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0: {'int1': <tf.Tensor: id=14934920, shape=(), dtype=float32, numpy=-0.15933833>, 'int2': <tf.Tensor: id=14934925, shape=(), dtype=float32, numpy=-0.26784092>, 'int3': <tf.Tensor: id=14934926, shape=(), dtype=float32, numpy=-0.052548945>, 'int4': <tf.Tensor: id=14934927, shape=(), dtype=float32, numpy=0.19075122>, 'int5': <tf.Tensor: id=14934928, shape=(), dtype=float32, numpy=-0.2663895>, 'int6': <tf.Tensor: id=14934929, shape=(), dtype=float32, numpy=-0.27985165>, 'int7': <tf.Tensor: id=14934930, shape=(), dtype=float32, numpy=-0.21700503>, 'int8': <tf.Tensor: id=14934931, shape=(), dtype=float32, numpy=-0.39050195>, 'int9': <tf.Tensor: id=14934932, shape=(), dtype=float32, numpy=-0.44084102>, 'int10': <tf.Tensor: id=14934921, shape=(), dtype=float32, numpy=0.5591261>, 'int11': <tf.Tensor: id=14934922, shape=(), dtype=float32, numpy=-0.33329692>, 'int12': <tf.Tensor: id=14934923, shape=(), dtype=float32, numpy=0.0>, 'int13': <tf.Tensor: id=14934924, shape=(), dtype=float32, numpy=0.048269283>, 'cat1': <tf.Tensor: id=14934894, shape=(), dtype=string, numpy=b'5a9ed9b0'>, 'cat2': <tf.Tensor: id=14934905, shape=(), dtype=string, numpy=b'80e26c9b'>, 'cat3': <tf.Tensor: id=14934913, shape=(), dtype=string, numpy=b'b4075e9c'>, 'cat4': <tf.Tensor: id=14934914, shape=(), dtype=string, numpy=b'85dd697c'>, 'cat5': <tf.Tensor: id=14934915, shape=(), dtype=string, numpy=b'4cf72387'>, 'cat6': <tf.Tensor: id=14934916, shape=(), dtype=string, numpy=b''>, 'cat7': <tf.Tensor: id=14934917, shape=(), dtype=string, numpy=b'a56b946e'>, 'cat8': <tf.Tensor: id=14934918, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=14934919, shape=(), dtype=string, numpy=b'a73ee510'>, 'cat10': <tf.Tensor: id=14934895, shape=(), dtype=string, numpy=b'64270d78'>, 'cat11': <tf.Tensor: id=14934896, shape=(), dtype=string, numpy=b'3898d718'>, 'cat12': <tf.Tensor: id=14934897, shape=(), dtype=string, numpy=b'c3f48f4c'>, 'cat13': <tf.Tensor: id=14934898, shape=(), dtype=string, numpy=b'86462f28'>, 'cat14': <tf.Tensor: id=14934899, shape=(), dtype=string, numpy=b'07d13a8f'>, 'cat15': <tf.Tensor: id=14934900, shape=(), dtype=string, numpy=b'e8f4b767'>, 'cat16': <tf.Tensor: id=14934901, shape=(), dtype=string, numpy=b'2d0bbe92'>, 'cat17': <tf.Tensor: id=14934902, shape=(), dtype=string, numpy=b'07c540c4'>, 'cat18': <tf.Tensor: id=14934903, shape=(), dtype=string, numpy=b'005c6740'>, 'cat19': <tf.Tensor: id=14934904, shape=(), dtype=string, numpy=b'21ddcdc9'>, 'cat20': <tf.Tensor: id=14934906, shape=(), dtype=string, numpy=b'5840adea'>, 'cat21': <tf.Tensor: id=14934907, shape=(), dtype=string, numpy=b'12322e9d'>, 'cat22': <tf.Tensor: id=14934908, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=14934909, shape=(), dtype=string, numpy=b'32c7478e'>, 'cat24': <tf.Tensor: id=14934910, shape=(), dtype=string, numpy=b'1793a828'>, 'cat25': <tf.Tensor: id=14934911, shape=(), dtype=string, numpy=b'e8b83407'>, 'cat26': <tf.Tensor: id=14934912, shape=(), dtype=string, numpy=b'b9809574'>} \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'python_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-f4927cca9dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mrow_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransofrom_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'python_function'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "\n",
    "@tf.function\n",
    "def transoform_row(row_dict, mean_dict, std_dict):\n",
    "  #dict_without_label = row_dict.copy() - OrderedDict.copy does not work in AutoGraph\n",
    "  dict_without_label = dict(row_dict)\n",
    "  label = dict_without_label.pop('label')\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] != 0:\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        else:\n",
    "            dict_without_label[field.name] = 0.0 # don't use normalized 0 value for nulls\n",
    "  return (dict_without_label, label)\n",
    "           \n",
    "def read_gcs(table_name, **kwargs):\n",
    "  gcs_filename_glob = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/{}*'.format(table_name)\n",
    "  selected_columns = list(field.name for field in CSV_SCHEMA)\n",
    "  column_names = selected_columns + ['row_hash']\n",
    "\n",
    "  print(column_names)\n",
    "\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      gcs_filename_glob,\n",
    "      batch_size=1,\n",
    "      column_names = column_names,\n",
    "      select_columns = selected_columns,\n",
    "      num_epochs=1,\n",
    "      field_delim='\\t',\n",
    "      header=False,\n",
    "      ignore_errors=False,\n",
    "      **kwargs)\n",
    "\n",
    "    #.apply(tf.data.experimental.unbatch()) \\\n",
    "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "  transformed_ds = dataset.unbatch().map (lambda row: transofrom_row(row, mean_dict, std_dict))\n",
    "\n",
    "  return transformed_ds\n",
    "\n",
    "dataset = read_gcs('test_small')\n",
    "row_index = 0\n",
    "for row,label in dataset.take(1):\n",
    "  print(\"row %d: %s \\n\\n\" % (row_index, row))\n",
    "  row_index += 1\n",
    "    \n",
    "tf.autograph.to_code(transofrom_row.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0: ({'int1': <tf.Tensor: id=14936090, shape=(), dtype=float32, numpy=-0.053283405>, 'int2': <tf.Tensor: id=14936095, shape=(), dtype=float32, numpy=0.0>, 'int3': <tf.Tensor: id=14936096, shape=(), dtype=float32, numpy=-0.05003621>, 'int4': <tf.Tensor: id=14936097, shape=(), dtype=float32, numpy=0.53192276>, 'int5': <tf.Tensor: id=14936098, shape=(), dtype=float32, numpy=-0.26637506>, 'int6': <tf.Tensor: id=14936099, shape=(), dtype=float32, numpy=-0.25632632>, 'int7': <tf.Tensor: id=14936100, shape=(), dtype=float32, numpy=-0.17158473>, 'int8': <tf.Tensor: id=14936101, shape=(), dtype=float32, numpy=0.44837976>, 'int9': <tf.Tensor: id=14936102, shape=(), dtype=float32, numpy=-0.17300385>, 'int10': <tf.Tensor: id=14936091, shape=(), dtype=float32, numpy=0.5591261>, 'int11': <tf.Tensor: id=14936092, shape=(), dtype=float32, numpy=-0.14095487>, 'int12': <tf.Tensor: id=14936093, shape=(), dtype=float32, numpy=0.0>, 'int13': <tf.Tensor: id=14936094, shape=(), dtype=float32, numpy=0.23331816>, 'cat1': <tf.Tensor: id=14936064, shape=(), dtype=string, numpy=b'8cf07265'>, 'cat2': <tf.Tensor: id=14936075, shape=(), dtype=string, numpy=b'38a947a1'>, 'cat3': <tf.Tensor: id=14936083, shape=(), dtype=string, numpy=b'642efc0f'>, 'cat4': <tf.Tensor: id=14936084, shape=(), dtype=string, numpy=b'48ee52c9'>, 'cat5': <tf.Tensor: id=14936085, shape=(), dtype=string, numpy=b'25c83c98'>, 'cat6': <tf.Tensor: id=14936086, shape=(), dtype=string, numpy=b'7e0ccccf'>, 'cat7': <tf.Tensor: id=14936087, shape=(), dtype=string, numpy=b'c7575e05'>, 'cat8': <tf.Tensor: id=14936088, shape=(), dtype=string, numpy=b'0b153874'>, 'cat9': <tf.Tensor: id=14936089, shape=(), dtype=string, numpy=b'a73ee510'>, 'cat10': <tf.Tensor: id=14936065, shape=(), dtype=string, numpy=b'e286f1e6'>, 'cat11': <tf.Tensor: id=14936066, shape=(), dtype=string, numpy=b'fd7856c1'>, 'cat12': <tf.Tensor: id=14936067, shape=(), dtype=string, numpy=b'7cad8267'>, 'cat13': <tf.Tensor: id=14936068, shape=(), dtype=string, numpy=b'6a430a5b'>, 'cat14': <tf.Tensor: id=14936069, shape=(), dtype=string, numpy=b'b28479f6'>, 'cat15': <tf.Tensor: id=14936070, shape=(), dtype=string, numpy=b'4e8979f6'>, 'cat16': <tf.Tensor: id=14936071, shape=(), dtype=string, numpy=b'da76182e'>, 'cat17': <tf.Tensor: id=14936072, shape=(), dtype=string, numpy=b'e5ba7672'>, 'cat18': <tf.Tensor: id=14936073, shape=(), dtype=string, numpy=b'002c3270'>, 'cat19': <tf.Tensor: id=14936074, shape=(), dtype=string, numpy=b''>, 'cat20': <tf.Tensor: id=14936076, shape=(), dtype=string, numpy=b''>, 'cat21': <tf.Tensor: id=14936077, shape=(), dtype=string, numpy=b'f492dbbf'>, 'cat22': <tf.Tensor: id=14936078, shape=(), dtype=string, numpy=b''>, 'cat23': <tf.Tensor: id=14936079, shape=(), dtype=string, numpy=b'3a171ecb'>, 'cat24': <tf.Tensor: id=14936080, shape=(), dtype=string, numpy=b'ba02f03a'>, 'cat25': <tf.Tensor: id=14936081, shape=(), dtype=string, numpy=b''>, 'cat26': <tf.Tensor: id=14936082, shape=(), dtype=string, numpy=b''>}, <tf.Tensor: id=14936103, shape=(), dtype=int32, numpy=0>) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_row(row_dict, mean_dict, std_dict):\n",
    "  #dict_without_label = row_dict.copy() - OrderedDict.copy does not work in AutoGraph\n",
    "  dict_without_label = dict(row_dict)\n",
    "  label = dict_without_label.pop('label')\n",
    "  for field in CSV_SCHEMA:\n",
    "    if (field.name.startswith('int')):\n",
    "        if dict_without_label[field.name] != 0:\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        else:\n",
    "            dict_without_label[field.name] = 0.0 # don't use normalized 0 value for nulls\n",
    "  return (dict_without_label, label)\n",
    "\n",
    "def transofrom_row_gcs(row_tuple, mean_dict, std_dict):\n",
    "    row_dict = dict(zip(list(field.name for field in CSV_SCHEMA) + ['row_hash'], list(row_tuple)))\n",
    "    row_dict.pop('row_hash')\n",
    "    return transform_row(row_dict, mean_dict, std_dict)\n",
    "\n",
    "def read_gcs(table_name):\n",
    "  gcs_filename_glob = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/{}.csv'.format(table_name)\n",
    "  record_defaults = list(tf.int32 if field.name == 'label' else tf.constant(0, dtype=tf.int32) if field.name.startswith('int') else tf.constant('', dtype=tf.string) for field in CSV_SCHEMA) + [tf.string]\n",
    "  dataset = tf.data.experimental.CsvDataset(\n",
    "      gcs_filename_glob,\n",
    "      record_defaults,\n",
    "      field_delim='\\t',\n",
    "      header=False)\n",
    "  (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "  transformed_ds = dataset.map (lambda *row_tuple: transofrom_row_gcs(row_tuple, mean_dict, std_dict))\n",
    "  return transformed_ds\n",
    "\n",
    "dataset = read_gcs('test_small')\n",
    "row_index = 0\n",
    "for row in dataset.take(1):\n",
    "  print(\"row %d: %s \\n\\n\" % (row_index, row))\n",
    "  row_index += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "tpl = (1, 3, 6)\n",
    "\n",
    "def print_tpl(tpla):\n",
    "    print(tpla)\n",
    "    \n",
    "print_tpl(tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t3\t0\t7\t12\t54\t18\t5\t20\t68\t1\t2\t0\t12\t8cf07265\t38a947a1\t642efc0f\t48ee52c9\t25c83c98\t7e0ccccf\tc7575e05\t0b153874\ta73ee510\te286f1e6\tfd7856c1\t7cad8267\t6a430a5b\tb28479f6\t4e8979f6\tda76182e\te5ba7672\t002c3270\t\t\tf492dbbf\t\t3a171ecb\tba02f03a\t\t\t6264987978780750083\r\n",
      "0\t\t0\t3\t1\t266290\t\t\t2\t\t\t\t\t1\t05db9164\t38a947a1\t78a10995\ta5a18f25\t4cf72387\t7e0ccccf\t4b3c7cfe\t51d76abe\t7cc72ec2\tf6fd64a2\t8b94178b\ta79b473e\t025225f2\tb28479f6\t4f047de8\t9f3b50db\t07c540c4\t002c3270\t\t\tf14a2f09\tad3062eb\t32c7478e\tba02f03a\t\t\t3598378017670020087\r\n",
      "0\t1\t4"
     ]
    }
   ],
   "source": [
    "!gsutil cat -r 0-500 gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/test_small.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
